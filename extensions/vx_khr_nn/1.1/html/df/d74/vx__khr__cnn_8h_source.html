<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.6"/>
<title>OpenVX Neural Network Extension: vx_khr_cnn.h Source File</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../OpenVX_Color_119x55.png"/></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname">OpenVX Neural Network Extension
   &#160;<span id="projectnumber">7505566</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.6 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="../../index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="../../modules.html"><span>Modules</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="../../search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('df/d74/vx__khr__cnn_8h_source.html','../../');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Groups</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">vx_khr_cnn.h</div>  </div>
</div><!--header-->
<div class="contents">
<div class="fragment"><div class="line"><a name="l00001"></a><span class="lineno">    1</span>&#160;<span class="comment">/* </span></div>
<div class="line"><a name="l00002"></a><span class="lineno">    2</span>&#160;<span class="comment"> * Copyright (c) 2012-2016 The Khronos Group Inc.</span></div>
<div class="line"><a name="l00003"></a><span class="lineno">    3</span>&#160;<span class="comment"> *</span></div>
<div class="line"><a name="l00004"></a><span class="lineno">    4</span>&#160;<span class="comment"> * Permission is hereby granted, free of charge, to any person obtaining a</span></div>
<div class="line"><a name="l00005"></a><span class="lineno">    5</span>&#160;<span class="comment"> * copy of this software and/or associated documentation files (the</span></div>
<div class="line"><a name="l00006"></a><span class="lineno">    6</span>&#160;<span class="comment"> * &quot;Materials&quot;), to deal in the Materials without restriction, including</span></div>
<div class="line"><a name="l00007"></a><span class="lineno">    7</span>&#160;<span class="comment"> * without limitation the rights to use, copy, modify, merge, publish,</span></div>
<div class="line"><a name="l00008"></a><span class="lineno">    8</span>&#160;<span class="comment"> * distribute, sublicense, and/or sell copies of the Materials, and to</span></div>
<div class="line"><a name="l00009"></a><span class="lineno">    9</span>&#160;<span class="comment"> * permit persons to whom the Materials are furnished to do so, subject to</span></div>
<div class="line"><a name="l00010"></a><span class="lineno">   10</span>&#160;<span class="comment"> * the following conditions:</span></div>
<div class="line"><a name="l00011"></a><span class="lineno">   11</span>&#160;<span class="comment"> *</span></div>
<div class="line"><a name="l00012"></a><span class="lineno">   12</span>&#160;<span class="comment"> * The above copyright notice and this permission notice shall be included</span></div>
<div class="line"><a name="l00013"></a><span class="lineno">   13</span>&#160;<span class="comment"> * in all copies or substantial portions of the Materials.</span></div>
<div class="line"><a name="l00014"></a><span class="lineno">   14</span>&#160;<span class="comment"> *</span></div>
<div class="line"><a name="l00015"></a><span class="lineno">   15</span>&#160;<span class="comment"> * MODIFICATIONS TO THIS FILE MAY MEAN IT NO LONGER ACCURATELY REFLECTS</span></div>
<div class="line"><a name="l00016"></a><span class="lineno">   16</span>&#160;<span class="comment"> * KHRONOS STANDARDS. THE UNMODIFIED, NORMATIVE VERSIONS OF KHRONOS</span></div>
<div class="line"><a name="l00017"></a><span class="lineno">   17</span>&#160;<span class="comment"> * SPECIFICATIONS AND HEADER INFORMATION ARE LOCATED AT</span></div>
<div class="line"><a name="l00018"></a><span class="lineno">   18</span>&#160;<span class="comment"> *    https://www.khronos.org/registry/</span></div>
<div class="line"><a name="l00019"></a><span class="lineno">   19</span>&#160;<span class="comment"> *</span></div>
<div class="line"><a name="l00020"></a><span class="lineno">   20</span>&#160;<span class="comment"> * THE MATERIALS ARE PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND,</span></div>
<div class="line"><a name="l00021"></a><span class="lineno">   21</span>&#160;<span class="comment"> * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF</span></div>
<div class="line"><a name="l00022"></a><span class="lineno">   22</span>&#160;<span class="comment"> * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.</span></div>
<div class="line"><a name="l00023"></a><span class="lineno">   23</span>&#160;<span class="comment"> * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY</span></div>
<div class="line"><a name="l00024"></a><span class="lineno">   24</span>&#160;<span class="comment"> * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,</span></div>
<div class="line"><a name="l00025"></a><span class="lineno">   25</span>&#160;<span class="comment"> * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE</span></div>
<div class="line"><a name="l00026"></a><span class="lineno">   26</span>&#160;<span class="comment"> * MATERIALS OR THE USE OR OTHER DEALINGS IN THE MATERIALS.</span></div>
<div class="line"><a name="l00027"></a><span class="lineno">   27</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00028"></a><span class="lineno">   28</span>&#160;</div>
<div class="line"><a name="l00029"></a><span class="lineno">   29</span>&#160;<span class="preprocessor">#ifndef _VX_KHR_CNN_H_</span></div>
<div class="line"><a name="l00030"></a><span class="lineno">   30</span>&#160;<span class="preprocessor"></span><span class="preprocessor">#define _VX_KHR_CNN_H_</span></div>
<div class="line"><a name="l00031"></a><span class="lineno">   31</span>&#160;<span class="preprocessor"></span><span class="comment"></span></div>
<div class="line"><a name="l00032"></a><span class="lineno">   32</span>&#160;<span class="comment">/*!</span></div>
<div class="line"><a name="l00033"></a><span class="lineno">   33</span>&#160;<span class="comment"> * \file</span></div>
<div class="line"><a name="l00034"></a><span class="lineno">   34</span>&#160;<span class="comment"> * \brief The Khronos Extension for Deep Convolutional Networks Functions.</span></div>
<div class="line"><a name="l00035"></a><span class="lineno">   35</span>&#160;<span class="comment"> *</span></div>
<div class="line"><a name="l00036"></a><span class="lineno">   36</span>&#160;<span class="comment"> * \defgroup group_cnn Extension: Deep Convolutional Networks API</span></div>
<div class="line"><a name="l00037"></a><span class="lineno">   37</span>&#160;<span class="comment"> * \brief Convolutional Network Nodes.</span></div>
<div class="line"><a name="l00038"></a><span class="lineno">   38</span>&#160;<span class="comment"> * \defgroup group_tensor Tensor API</span></div>
<div class="line"><a name="l00039"></a><span class="lineno">   39</span>&#160;<span class="comment"> * \brief The Tensor API for Deep Convolutional Networks Functions.</span></div>
<div class="line"><a name="l00040"></a><span class="lineno">   40</span>&#160;<span class="comment"> * \details The tensor is a multidimensional opaque object.Since the object have no visibility to the programmer. Vendors can introduce many optimization possibilities.</span></div>
<div class="line"><a name="l00041"></a><span class="lineno">   41</span>&#160;<span class="comment"> * An example of such optimization can be found in the following article.http://arxiv.org/abs/1510.00149</span></div>
<div class="line"><a name="l00042"></a><span class="lineno">   42</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00043"></a><span class="lineno">   43</span>&#160;</div>
<div class="line"><a name="l00044"></a><span class="lineno">   44</span>&#160;<span class="preprocessor">#define OPENVX_KHR_CNN   &quot;vx_khr_cnn&quot;</span></div>
<div class="line"><a name="l00045"></a><span class="lineno">   45</span>&#160;<span class="preprocessor"></span></div>
<div class="line"><a name="l00046"></a><span class="lineno">   46</span>&#160;<span class="preprocessor">#if defined(OPENVX_CNN_1_0)</span></div>
<div class="line"><a name="l00047"></a><span class="lineno">   47</span>&#160;<span class="preprocessor"></span><span class="preprocessor">#undef OPENVX_CNN_1_1</span></div>
<div class="line"><a name="l00048"></a><span class="lineno">   48</span>&#160;<span class="preprocessor"></span><span class="preprocessor">#endif</span></div>
<div class="line"><a name="l00049"></a><span class="lineno">   49</span>&#160;<span class="preprocessor"></span></div>
<div class="line"><a name="l00050"></a><span class="lineno">   50</span>&#160;<span class="preprocessor">#include &lt;VX/vx.h&gt;</span></div>
<div class="line"><a name="l00051"></a><span class="lineno">   51</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00052"></a><span class="lineno">   52</span>&#160;<span class="comment">/*! \brief tensor Data attributes.</span></div>
<div class="line"><a name="l00053"></a><span class="lineno">   53</span>&#160;<span class="comment"> * \ingroup group_tensor</span></div>
<div class="line"><a name="l00054"></a><span class="lineno">   54</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00055"></a><span class="lineno"><a class="line" href="../../dd/dad/group__group__tensor.html#ga284570428744ab05195daa2707dcb688">   55</a></span>&#160;<span class="keyword">enum</span> <a class="code" href="../../dd/dad/group__group__tensor.html#ga284570428744ab05195daa2707dcb688">vx_tensor_attribute_e</a></div>
<div class="line"><a name="l00056"></a><span class="lineno">   56</span>&#160;{<span class="comment"></span></div>
<div class="line"><a name="l00057"></a><span class="lineno">   57</span>&#160;<span class="comment">    /*! \brief Number of dimensions. */</span></div>
<div class="line"><a name="l00058"></a><span class="lineno"><a class="line" href="../../dd/dad/group__group__tensor.html#gga284570428744ab05195daa2707dcb688a0930a6e574a2c12049d60c0daea43778">   58</a></span>&#160;    <a class="code" href="../../dd/dad/group__group__tensor.html#gga284570428744ab05195daa2707dcb688a0930a6e574a2c12049d60c0daea43778">VX_TENSOR_NUM_OF_DIMS</a> = VX_ATTRIBUTE_BASE( VX_ID_KHRONOS, VX_TYPE_TENSOR ) + 0x0,<span class="comment"></span></div>
<div class="line"><a name="l00059"></a><span class="lineno">   59</span>&#160;<span class="comment">    /*! \brief Dimension sizes. */</span></div>
<div class="line"><a name="l00060"></a><span class="lineno"><a class="line" href="../../dd/dad/group__group__tensor.html#gga284570428744ab05195daa2707dcb688a589cd47c54ab8a962b46ae0f8a9e67be">   60</a></span>&#160;    <a class="code" href="../../dd/dad/group__group__tensor.html#gga284570428744ab05195daa2707dcb688a589cd47c54ab8a962b46ae0f8a9e67be">VX_TENSOR_DIMS</a>        = VX_ATTRIBUTE_BASE( VX_ID_KHRONOS, VX_TYPE_TENSOR ) + 0x1,<span class="comment"></span></div>
<div class="line"><a name="l00061"></a><span class="lineno">   61</span>&#160;<span class="comment">    /*! \brief tensor Data element data type. &lt;tt&gt;vx_type_e&lt;/tt&gt; */</span></div>
<div class="line"><a name="l00062"></a><span class="lineno"><a class="line" href="../../dd/dad/group__group__tensor.html#gga284570428744ab05195daa2707dcb688a69071022451108c3e0dd6e6c978f30f7">   62</a></span>&#160;    <a class="code" href="../../dd/dad/group__group__tensor.html#gga284570428744ab05195daa2707dcb688a69071022451108c3e0dd6e6c978f30f7">VX_TENSOR_DATA_TYPE</a>   = VX_ATTRIBUTE_BASE( VX_ID_KHRONOS, VX_TYPE_TENSOR ) + 0x2,<span class="comment"></span></div>
<div class="line"><a name="l00063"></a><span class="lineno">   63</span>&#160;<span class="comment">    /*! \brief fixed point position when the input element type is int16. */</span></div>
<div class="line"><a name="l00064"></a><span class="lineno"><a class="line" href="../../dd/dad/group__group__tensor.html#gga284570428744ab05195daa2707dcb688aba74715b3a066ea13959fda51cb7cb91">   64</a></span>&#160;    <a class="code" href="../../dd/dad/group__group__tensor.html#gga284570428744ab05195daa2707dcb688aba74715b3a066ea13959fda51cb7cb91">VX_TENSOR_FIXED_POINT_POS</a> = VX_ATTRIBUTE_BASE(VX_ID_KHRONOS, VX_TYPE_TENSOR) + 0x4</div>
<div class="line"><a name="l00065"></a><span class="lineno">   65</span>&#160;};</div>
<div class="line"><a name="l00066"></a><span class="lineno">   66</span>&#160;</div>
<div class="line"><a name="l00067"></a><span class="lineno">   67</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00068"></a><span class="lineno">   68</span>&#160;<span class="comment">/*! \brief A list of context attributes.</span></div>
<div class="line"><a name="l00069"></a><span class="lineno">   69</span>&#160;<span class="comment">* \ingroup group_tensor</span></div>
<div class="line"><a name="l00070"></a><span class="lineno">   70</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00071"></a><span class="lineno"><a class="line" href="../../dd/dad/group__group__tensor.html#ga7f849c7db71cc56b2a554d7d342ff421">   71</a></span>&#160;<span class="keyword">enum</span> <a class="code" href="../../dd/dad/group__group__tensor.html#ga7f849c7db71cc56b2a554d7d342ff421">vx_context_attribute_e</a> {<span class="comment"></span></div>
<div class="line"><a name="l00072"></a><span class="lineno">   72</span>&#160;<span class="comment">    /*! \brief tensor Data max num of dimensions supported by HW. */</span></div>
<div class="line"><a name="l00073"></a><span class="lineno"><a class="line" href="../../dd/dad/group__group__tensor.html#gga7f849c7db71cc56b2a554d7d342ff421af8fd07fd2755ca696d0cc50cbf1f67fb">   73</a></span>&#160;    <a class="code" href="../../dd/dad/group__group__tensor.html#gga7f849c7db71cc56b2a554d7d342ff421af8fd07fd2755ca696d0cc50cbf1f67fb">VX_CONTEXT_MAX_TENSOR_DIMENSIONS</a> = VX_ATTRIBUTE_BASE(VX_ID_KHRONOS, VX_TYPE_CONTEXT) + 0x0<span class="comment"></span></div>
<div class="line"><a name="l00074"></a><span class="lineno">   74</span>&#160;<span class="comment">    /*! \brief max accumulation bits supported */</span></div>
<div class="line"><a name="l00075"></a><span class="lineno">   75</span>&#160;    VX_CONTEXT_MAX_CONVOLUTIONAL_NETWORK_ACCUMULATION_BITS = VX_ATTRIBUTE_BASE(VX_ID_KHRONOS, VX_TYPE_CONTEXT) + 0x1<span class="comment"></span></div>
<div class="line"><a name="l00076"></a><span class="lineno">   76</span>&#160;<span class="comment">    /*! \brief min accumulation bits supported */</span></div>
<div class="line"><a name="l00077"></a><span class="lineno">   77</span>&#160;    VX_CONTEXT_MIN_CONVOLUTIONAL_NETWORK_ACCUMULATION_BITS = VX_ATTRIBUTE_BASE(VX_ID_KHRONOS, VX_TYPE_CONTEXT) + 0x2</div>
<div class="line"><a name="l00078"></a><span class="lineno">   78</span>&#160;};</div>
<div class="line"><a name="l00079"></a><span class="lineno">   79</span>&#160;</div>
<div class="line"><a name="l00080"></a><span class="lineno">   80</span>&#160;</div>
<div class="line"><a name="l00081"></a><span class="lineno">   81</span>&#160;<span class="comment">/*==============================================================================</span></div>
<div class="line"><a name="l00082"></a><span class="lineno">   82</span>&#160;<span class="comment">CONVOLUTIONAL_NETWORK structs and enums</span></div>
<div class="line"><a name="l00083"></a><span class="lineno">   83</span>&#160;<span class="comment">=============================================================================*/</span><span class="comment"></span></div>
<div class="line"><a name="l00084"></a><span class="lineno">   84</span>&#160;<span class="comment">/*! \brief The multidimensional data object (Tensor).</span></div>
<div class="line"><a name="l00085"></a><span class="lineno">   85</span>&#160;<span class="comment"> * \see vxCreateTensor</span></div>
<div class="line"><a name="l00086"></a><span class="lineno">   86</span>&#160;<span class="comment"> * \ingroup group_tensor</span></div>
<div class="line"><a name="l00087"></a><span class="lineno">   87</span>&#160;<span class="comment"> * \extends vx_reference</span></div>
<div class="line"><a name="l00088"></a><span class="lineno">   88</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00089"></a><span class="lineno"><a class="line" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">   89</a></span>&#160;<span class="keyword">typedef</span> <span class="keyword">struct </span>_vx_tensor_t * <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>;</div>
<div class="line"><a name="l00090"></a><span class="lineno">   90</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00091"></a><span class="lineno">   91</span>&#160;<span class="comment">/*! \brief The multi dimensional view data structure.</span></div>
<div class="line"><a name="l00092"></a><span class="lineno">   92</span>&#160;<span class="comment">* \details Used to split tensors into several views. Or concatenate several view into one tensor.</span></div>
<div class="line"><a name="l00093"></a><span class="lineno">   93</span>&#160;<span class="comment">* \see vxCreateTensorFromView</span></div>
<div class="line"><a name="l00094"></a><span class="lineno">   94</span>&#160;<span class="comment">* \ingroup group_tensor</span></div>
<div class="line"><a name="l00095"></a><span class="lineno">   95</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00096"></a><span class="lineno"><a class="line" href="../../dd/dad/group__group__tensor.html#ga372396aa9489f7fe30f9af0384d18340">   96</a></span>&#160;<span class="keyword">typedef</span> <span class="keyword">struct </span>_vx_tensor_view_t * <a class="code" href="../../dd/dad/group__group__tensor.html#ga372396aa9489f7fe30f9af0384d18340">vx_tensor_view</a>;</div>
<div class="line"><a name="l00097"></a><span class="lineno">   97</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00098"></a><span class="lineno">   98</span>&#160;<span class="comment">/*! \brief The addressing of a tensor view patch structure is used by the Host only</span></div>
<div class="line"><a name="l00099"></a><span class="lineno">   99</span>&#160;<span class="comment">* to address elements in a tensor view patch.</span></div>
<div class="line"><a name="l00100"></a><span class="lineno">  100</span>&#160;<span class="comment">* \see &lt;tt&gt;\ref vxCopyTensorPatch&lt;/tt&gt;</span></div>
<div class="line"><a name="l00101"></a><span class="lineno">  101</span>&#160;<span class="comment">* \ingroup group_tensor</span></div>
<div class="line"><a name="l00102"></a><span class="lineno">  102</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00103"></a><span class="lineno"><a class="line" href="../../dd/dad/group__group__tensor.html#ga6f6c0db1e3f276331707a50522294e51">  103</a></span>&#160;<span class="keyword">typedef</span> <span class="keyword">struct </span>_vx_tensor_addressing_t  * <a class="code" href="../../dd/dad/group__group__tensor.html#ga6f6c0db1e3f276331707a50522294e51">vx_tensor_addressing</a>;</div>
<div class="line"><a name="l00104"></a><span class="lineno">  104</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00105"></a><span class="lineno">  105</span>&#160;<span class="comment">/*! \brief The Convolutional Network down scaling size rounding type list.</span></div>
<div class="line"><a name="l00106"></a><span class="lineno">  106</span>&#160;<span class="comment">* \details rounding done downscaling, In convolution and pooling functions.</span></div>
<div class="line"><a name="l00107"></a><span class="lineno">  107</span>&#160;<span class="comment">* Relevant when input size is even.</span></div>
<div class="line"><a name="l00108"></a><span class="lineno">  108</span>&#160;<span class="comment">* \ingroup group_cnn</span></div>
<div class="line"><a name="l00109"></a><span class="lineno">  109</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00110"></a><span class="lineno"><a class="line" href="../../d6/d9a/group__group__cnn.html#gaea14552ad4e594583c11d8e6163557c1">  110</a></span>&#160;<span class="keyword">enum</span> <a class="code" href="../../d6/d9a/group__group__cnn.html#gaea14552ad4e594583c11d8e6163557c1">vx_convolutional_networks_rounding_type_e</a></div>
<div class="line"><a name="l00111"></a><span class="lineno">  111</span>&#160;{<span class="comment"></span></div>
<div class="line"><a name="l00112"></a><span class="lineno">  112</span>&#160;<span class="comment">    /*! \brief floor rounding  */</span></div>
<div class="line"><a name="l00113"></a><span class="lineno"><a class="line" href="../../d6/d9a/group__group__cnn.html#ggaea14552ad4e594583c11d8e6163557c1a71594a34842bc09f8f876809d410cb52">  113</a></span>&#160;    <a class="code" href="../../d6/d9a/group__group__cnn.html#ggaea14552ad4e594583c11d8e6163557c1a71594a34842bc09f8f876809d410cb52">VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR</a> = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_ROUNDING_TYPE) + 0x0,<span class="comment"></span></div>
<div class="line"><a name="l00114"></a><span class="lineno">  114</span>&#160;<span class="comment">    /*! \brief ceil rounding */</span></div>
<div class="line"><a name="l00115"></a><span class="lineno"><a class="line" href="../../d6/d9a/group__group__cnn.html#ggaea14552ad4e594583c11d8e6163557c1a0548ecdc1205f9d8031fc557e41bf2f2">  115</a></span>&#160;    <a class="code" href="../../d6/d9a/group__group__cnn.html#ggaea14552ad4e594583c11d8e6163557c1a0548ecdc1205f9d8031fc557e41bf2f2">VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING</a> = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_ROUNDING_TYPE) + 0x1</div>
<div class="line"><a name="l00116"></a><span class="lineno">  116</span>&#160;};</div>
<div class="line"><a name="l00117"></a><span class="lineno">  117</span>&#160;</div>
<div class="line"><a name="l00118"></a><span class="lineno">  118</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00119"></a><span class="lineno">  119</span>&#160;<span class="comment">/*! \brief The Convolutional Network pooling type list.</span></div>
<div class="line"><a name="l00120"></a><span class="lineno">  120</span>&#160;<span class="comment">* \details kind of pooling done in pooling function</span></div>
<div class="line"><a name="l00121"></a><span class="lineno">  121</span>&#160;<span class="comment">* \ingroup group_cnn</span></div>
<div class="line"><a name="l00122"></a><span class="lineno">  122</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00123"></a><span class="lineno"><a class="line" href="../../d6/d9a/group__group__cnn.html#gab041071cfd76f089eba37379db229fb7">  123</a></span>&#160;<span class="keyword">enum</span> <a class="code" href="../../d6/d9a/group__group__cnn.html#gab041071cfd76f089eba37379db229fb7">vx_convolutional_network_pooling_type_e</a></div>
<div class="line"><a name="l00124"></a><span class="lineno">  124</span>&#160;{<span class="comment"></span></div>
<div class="line"><a name="l00125"></a><span class="lineno">  125</span>&#160;<span class="comment">    /*! \brief max pooling*/</span></div>
<div class="line"><a name="l00126"></a><span class="lineno"><a class="line" href="../../d6/d9a/group__group__cnn.html#ggab041071cfd76f089eba37379db229fb7a530049de61a14d590562f8840a19be2e">  126</a></span>&#160;    <a class="code" href="../../d6/d9a/group__group__cnn.html#ggab041071cfd76f089eba37379db229fb7a530049de61a14d590562f8840a19be2e">VX_CONVOLUTIONAL_NETWORK_POOLING_MAX</a> = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_POOL_TYPE) + 0x0,<span class="comment"></span></div>
<div class="line"><a name="l00127"></a><span class="lineno">  127</span>&#160;<span class="comment">    /*! \brief average pooling*/</span></div>
<div class="line"><a name="l00128"></a><span class="lineno"><a class="line" href="../../d6/d9a/group__group__cnn.html#ggab041071cfd76f089eba37379db229fb7ac486a0bcf89d55a1a0bfffff548aad76">  128</a></span>&#160;    <a class="code" href="../../d6/d9a/group__group__cnn.html#ggab041071cfd76f089eba37379db229fb7ac486a0bcf89d55a1a0bfffff548aad76">VX_CONVOLUTIONAL_NETWORK_POOLING_AVG</a> = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_POOL_TYPE) + 0x1</div>
<div class="line"><a name="l00129"></a><span class="lineno">  129</span>&#160;};</div>
<div class="line"><a name="l00130"></a><span class="lineno">  130</span>&#160;</div>
<div class="line"><a name="l00131"></a><span class="lineno">  131</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00132"></a><span class="lineno">  132</span>&#160;<span class="comment">/*! \brief The Convolutional Network normalization type list.</span></div>
<div class="line"><a name="l00133"></a><span class="lineno">  133</span>&#160;<span class="comment">* \ingroup group_cnn</span></div>
<div class="line"><a name="l00134"></a><span class="lineno">  134</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00135"></a><span class="lineno"><a class="line" href="../../d6/d9a/group__group__cnn.html#gad8a98bdea5d6620d3829b272bcccf9ab">  135</a></span>&#160;<span class="keyword">enum</span> <a class="code" href="../../d6/d9a/group__group__cnn.html#gad8a98bdea5d6620d3829b272bcccf9ab">vx_convolutional_network_norm_type_e</a></div>
<div class="line"><a name="l00136"></a><span class="lineno">  136</span>&#160;{<span class="comment"></span></div>
<div class="line"><a name="l00137"></a><span class="lineno">  137</span>&#160;<span class="comment">    /*! \brief normalization is done on same IFM*/</span></div>
<div class="line"><a name="l00138"></a><span class="lineno"><a class="line" href="../../d6/d9a/group__group__cnn.html#ggad8a98bdea5d6620d3829b272bcccf9abaaae1a24ba2c0456e0114c8526d3e550d">  138</a></span>&#160;    <a class="code" href="../../d6/d9a/group__group__cnn.html#ggad8a98bdea5d6620d3829b272bcccf9abaaae1a24ba2c0456e0114c8526d3e550d">VX_CONVOLUTIONAL_NETWORK_NORM_SAME_MAP</a> = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_NORM_TYPE) + 0x0,<span class="comment"></span></div>
<div class="line"><a name="l00139"></a><span class="lineno">  139</span>&#160;<span class="comment">    /*! \brief Normalization is done across different IFMs*/</span></div>
<div class="line"><a name="l00140"></a><span class="lineno"><a class="line" href="../../d6/d9a/group__group__cnn.html#ggad8a98bdea5d6620d3829b272bcccf9aba224c50ee37b0e4b1a8866c06939a4d95">  140</a></span>&#160;    <a class="code" href="../../d6/d9a/group__group__cnn.html#ggad8a98bdea5d6620d3829b272bcccf9aba224c50ee37b0e4b1a8866c06939a4d95">VX_CONVOLUTIONAL_NETWORK_NORM_ACROSS_MAPS</a> = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_NORM_TYPE) + 0x1,</div>
<div class="line"><a name="l00141"></a><span class="lineno">  141</span>&#160;};</div>
<div class="line"><a name="l00142"></a><span class="lineno">  142</span>&#160;</div>
<div class="line"><a name="l00143"></a><span class="lineno">  143</span>&#160;</div>
<div class="line"><a name="l00144"></a><span class="lineno">  144</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00145"></a><span class="lineno">  145</span>&#160;<span class="comment">/*! \brief The Convolutional Network activation functions list.</span></div>
<div class="line"><a name="l00146"></a><span class="lineno">  146</span>&#160;<span class="comment">* \details </span></div>
<div class="line"><a name="l00147"></a><span class="lineno">  147</span>&#160;<span class="comment">* &lt;table&gt;</span></div>
<div class="line"><a name="l00148"></a><span class="lineno">  148</span>&#160;<span class="comment">* &lt;tr&gt;&lt;td&gt; &lt;B&gt;Function name &lt;/B&gt; &lt;td&gt; &lt;B&gt;Mathematical definition&lt;/B&gt; &lt;td&gt; &lt;B&gt;Parameters&lt;/B&gt; &lt;td&gt; &lt;B&gt;Parameters type&lt;/B&gt;</span></div>
<div class="line"><a name="l00149"></a><span class="lineno">  149</span>&#160;<span class="comment">* &lt;tr&gt;&lt;td&gt;logistic &lt;td&gt; \f$f(x)=1/(1+e^{-x}) \f$  &lt;td&gt;  &lt;td&gt;</span></div>
<div class="line"><a name="l00150"></a><span class="lineno">  150</span>&#160;<span class="comment">* &lt;tr&gt;&lt;td&gt;hyperbolic tangent &lt;td&gt; \f$f(x)=a\cdot tanh(b\cdot x) \f$  &lt;td&gt; a,b  &lt;td&gt; VX_INT32</span></div>
<div class="line"><a name="l00151"></a><span class="lineno">  151</span>&#160;<span class="comment">* &lt;tr&gt;&lt;td&gt;relu &lt;td&gt; \f$f(x)=max(0,x)\f$  &lt;td&gt;  &lt;td&gt;</span></div>
<div class="line"><a name="l00152"></a><span class="lineno">  152</span>&#160;<span class="comment">* &lt;tr&gt;&lt;td&gt;bounded relu &lt;td&gt; \f$f(x)=min(a,max(0,x)) \f$  &lt;td&gt; a  &lt;td&gt; VX_INT32</span></div>
<div class="line"><a name="l00153"></a><span class="lineno">  153</span>&#160;<span class="comment">* &lt;tr&gt;&lt;td&gt;soft relu &lt;td&gt; \f$f(x)=log(1+e^{x}) \f$  &lt;td&gt;  &lt;td&gt;</span></div>
<div class="line"><a name="l00154"></a><span class="lineno">  154</span>&#160;<span class="comment">* &lt;tr&gt;&lt;td&gt;abs &lt;td&gt; \f$f(x)=\mid x\mid \f$  &lt;td&gt;  &lt;td&gt;</span></div>
<div class="line"><a name="l00155"></a><span class="lineno">  155</span>&#160;<span class="comment">* &lt;tr&gt;&lt;td&gt;square &lt;td&gt; \f$f(x)= x^2 \f$  &lt;td&gt;  &lt;td&gt;</span></div>
<div class="line"><a name="l00156"></a><span class="lineno">  156</span>&#160;<span class="comment">* &lt;tr&gt;&lt;td&gt;square root &lt;td&gt; \f$f(x)=\sqrt{x} \f$  &lt;td&gt;  &lt;td&gt;</span></div>
<div class="line"><a name="l00157"></a><span class="lineno">  157</span>&#160;<span class="comment">* &lt;tr&gt;&lt;td&gt;linear &lt;td&gt; \f$f(x)=ax+b \f$  &lt;td&gt;  a,b  &lt;td&gt; VX_INT32</span></div>
<div class="line"><a name="l00158"></a><span class="lineno">  158</span>&#160;<span class="comment">* &lt;/table&gt;</span></div>
<div class="line"><a name="l00159"></a><span class="lineno">  159</span>&#160;<span class="comment">* \ingroup group_cnn</span></div>
<div class="line"><a name="l00160"></a><span class="lineno">  160</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00161"></a><span class="lineno"><a class="line" href="../../d6/d9a/group__group__cnn.html#ga16743849220a5ac0539ed4272b33a7dc">  161</a></span>&#160;<span class="keyword">enum</span> <a class="code" href="../../d6/d9a/group__group__cnn.html#ga16743849220a5ac0539ed4272b33a7dc">vx_convolutional_network_activation_func_e</a></div>
<div class="line"><a name="l00162"></a><span class="lineno">  162</span>&#160;{</div>
<div class="line"><a name="l00163"></a><span class="lineno">  163</span>&#160;    VX_CONVOLUTIONAL_NETWORK_ACTIVATION_LOGISTIC = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC) + 0x0,</div>
<div class="line"><a name="l00164"></a><span class="lineno">  164</span>&#160;    VX_CONVOLUTIONAL_NETWORK_ACTIVATION_HYPERBOLIC_TAN = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC) + 0x1,</div>
<div class="line"><a name="l00165"></a><span class="lineno">  165</span>&#160;    VX_CONVOLUTIONAL_NETWORK_ACTIVATION_RELU = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC) + 0x2,</div>
<div class="line"><a name="l00166"></a><span class="lineno">  166</span>&#160;    VX_CONVOLUTIONAL_NETWORK_ACTIVATION_BRELU = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC) + 0x3,</div>
<div class="line"><a name="l00167"></a><span class="lineno">  167</span>&#160;    VX_CONVOLUTIONAL_NETWORK_ACTIVATION_SOFTRELU = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC) + 0x4,</div>
<div class="line"><a name="l00168"></a><span class="lineno">  168</span>&#160;    VX_CONVOLUTIONAL_NETWORK_ACTIVATION_ABS = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC) + 0x5,</div>
<div class="line"><a name="l00169"></a><span class="lineno">  169</span>&#160;    VX_CONVOLUTIONAL_NETWORK_ACTIVATION_SQUARE = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC) + 0x6,</div>
<div class="line"><a name="l00170"></a><span class="lineno">  170</span>&#160;    VX_CONVOLUTIONAL_NETWORK_ACTIVATION_SQRT = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC) + 0x7,</div>
<div class="line"><a name="l00171"></a><span class="lineno">  171</span>&#160;    VX_CONVOLUTIONAL_NETWORK_ACTIVATION_LINEAR = VX_ENUM_BASE(VX_ID_KHRONOS, VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC) + 0x8,</div>
<div class="line"><a name="l00172"></a><span class="lineno">  172</span>&#160;};</div>
<div class="line"><a name="l00173"></a><span class="lineno">  173</span>&#160;</div>
<div class="line"><a name="l00174"></a><span class="lineno">  174</span>&#160;<span class="comment">/* END CNN types*/</span></div>
<div class="line"><a name="l00175"></a><span class="lineno">  175</span>&#160;</div>
<div class="line"><a name="l00176"></a><span class="lineno">  176</span>&#160;<span class="comment">/*==============================================================================</span></div>
<div class="line"><a name="l00177"></a><span class="lineno">  177</span>&#160;<span class="comment">    TENSOR DATA FUNCTIONS</span></div>
<div class="line"><a name="l00178"></a><span class="lineno">  178</span>&#160;<span class="comment">=============================================================================*/</span><span class="comment"></span></div>
<div class="line"><a name="l00179"></a><span class="lineno">  179</span>&#160;<span class="comment">/*! \brief Creates an opaque reference to a tensor data buffer.</span></div>
<div class="line"><a name="l00180"></a><span class="lineno">  180</span>&#160;<span class="comment"> * \details Not guaranteed to exist until the &lt;tt&gt;vx_graph&lt;/tt&gt; containing it has been verified.</span></div>
<div class="line"><a name="l00181"></a><span class="lineno">  181</span>&#160;<span class="comment"> * \param [in] context The reference to the implementation context.</span></div>
<div class="line"><a name="l00182"></a><span class="lineno">  182</span>&#160;<span class="comment"> * \param [in] num_of_dims The number of dimensions.</span></div>
<div class="line"><a name="l00183"></a><span class="lineno">  183</span>&#160;<span class="comment"> * \param [in] sizes Dimensions sizes in elements.</span></div>
<div class="line"><a name="l00184"></a><span class="lineno">  184</span>&#160;<span class="comment"> * \param [in] data_format The &lt;tt&gt;vx_type_t&lt;/tt&gt; that represents the data type of the tensor data elements.</span></div>
<div class="line"><a name="l00185"></a><span class="lineno">  185</span>&#160;<span class="comment"> * \param [in] fixed_point_pos Specifies the fixed point position when the input element type is int16, if 0 calculations are performed in integer math</span></div>
<div class="line"><a name="l00186"></a><span class="lineno">  186</span>&#160;<span class="comment"> * \return A tensor data reference or zero when an error is encountered.</span></div>
<div class="line"><a name="l00187"></a><span class="lineno">  187</span>&#160;<span class="comment"> * \ingroup group_tensor</span></div>
<div class="line"><a name="l00188"></a><span class="lineno">  188</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00189"></a><span class="lineno">  189</span>&#160;VX_API_ENTRY <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#gaefbe70a40c695d79ebeb2bc550351e04">vxCreateTensor</a>(vx_context context, vx_uint32 num_of_dims, vx_uint32 *sizes, vx_enum data_format,vx_uint8 fixed_point_pos);</div>
<div class="line"><a name="l00190"></a><span class="lineno">  190</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00191"></a><span class="lineno">  191</span>&#160;<span class="comment">/*! \brief Creates an array of images into the multi-dimension data, this can be a adjacent 2D images or not depending on the stride value. </span></div>
<div class="line"><a name="l00192"></a><span class="lineno">  192</span>&#160;<span class="comment"> * The stride value is representing bytes in the third dimension.</span></div>
<div class="line"><a name="l00193"></a><span class="lineno">  193</span>&#160;<span class="comment"> * The OpenVX image object that points to a three dimension data and access it as an array of images.</span></div>
<div class="line"><a name="l00194"></a><span class="lineno">  194</span>&#160;<span class="comment"> * This has to be portion of the third lowest dimension, and the stride correspond to that third dimension.</span></div>
<div class="line"><a name="l00195"></a><span class="lineno">  195</span>&#160;<span class="comment"> * The returned Object array is an array of images. Where the image data is pointing to a specific memory in the input tensor.</span></div>
<div class="line"><a name="l00196"></a><span class="lineno">  196</span>&#160;<span class="comment"> * \param [in] tensor The tensor data from which to extract the images. Has to be a 3d tensor.</span></div>
<div class="line"><a name="l00197"></a><span class="lineno">  197</span>&#160;<span class="comment"> * \param [in] rect Image coordinates within tensor data.</span></div>
<div class="line"><a name="l00198"></a><span class="lineno">  198</span>&#160;<span class="comment"> * \param [in] array_size Number of images to extract.</span></div>
<div class="line"><a name="l00199"></a><span class="lineno">  199</span>&#160;<span class="comment"> * \param [in] stride Delta between two images in the array.</span></div>
<div class="line"><a name="l00200"></a><span class="lineno">  200</span>&#160;<span class="comment"> * \param [in] image_format The requested image format. Should match the tensor data&#39;s data type.</span></div>
<div class="line"><a name="l00201"></a><span class="lineno">  201</span>&#160;<span class="comment"> * \return An array of images pointing to the tensor data&#39;s data.</span></div>
<div class="line"><a name="l00202"></a><span class="lineno">  202</span>&#160;<span class="comment"> * \ingroup group_tensor</span></div>
<div class="line"><a name="l00203"></a><span class="lineno">  203</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00204"></a><span class="lineno">  204</span>&#160;VX_API_ENTRY vx_object_array VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#ga413cc779500ae059e474b1b58428c3f9">vxCreateImageObjectArrayFromTensor</a>(<a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> tensor, vx_rectangle_t rect, vx_uint32 array_size, vx_uint32 stride, vx_df_image image_format);</div>
<div class="line"><a name="l00205"></a><span class="lineno">  205</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00206"></a><span class="lineno">  206</span>&#160;<span class="comment">/*! \brief Creates a tensor data from another tensor data given a view. This second</span></div>
<div class="line"><a name="l00207"></a><span class="lineno">  207</span>&#160;<span class="comment"> * reference refers to the data in the original tensor data. Updates to this tensor data</span></div>
<div class="line"><a name="l00208"></a><span class="lineno">  208</span>&#160;<span class="comment"> * updates the parent tensor data. The view must be defined within the dimensions</span></div>
<div class="line"><a name="l00209"></a><span class="lineno">  209</span>&#160;<span class="comment"> * of the parent tensor data.</span></div>
<div class="line"><a name="l00210"></a><span class="lineno">  210</span>&#160;<span class="comment"> * \param [in] tensor The reference to the parent tensor data.</span></div>
<div class="line"><a name="l00211"></a><span class="lineno">  211</span>&#160;<span class="comment"> * \param [in] view The region of interest of a tensor view. Must contain points </span></div>
<div class="line"><a name="l00212"></a><span class="lineno">  212</span>&#160;<span class="comment"> * within the parent tensor data dimensions. &lt;tt&gt;\ref vx_tensor_view&lt;/tt&gt;</span></div>
<div class="line"><a name="l00213"></a><span class="lineno">  213</span>&#160;<span class="comment"> * \return The reference to the sub-tensor or zero if the view is invalid.</span></div>
<div class="line"><a name="l00214"></a><span class="lineno">  214</span>&#160;<span class="comment"> * \ingroup group_tensor</span></div>
<div class="line"><a name="l00215"></a><span class="lineno">  215</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00216"></a><span class="lineno">  216</span>&#160;VX_API_ENTRY <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#ga3f63a5a6f4a3a08eb45a35211956c44e">vxCreateTensorFromView</a>(<a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> tensor, <a class="code" href="../../dd/dad/group__group__tensor.html#ga372396aa9489f7fe30f9af0384d18340">vx_tensor_view</a> view);</div>
<div class="line"><a name="l00217"></a><span class="lineno">  217</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00218"></a><span class="lineno">  218</span>&#160;<span class="comment">/*! \brief Creates an opaque reference to a tensor data buffer with no direct</span></div>
<div class="line"><a name="l00219"></a><span class="lineno">  219</span>&#160;<span class="comment"> * user access. This function allows setting the tensor data dimensions or data format.</span></div>
<div class="line"><a name="l00220"></a><span class="lineno">  220</span>&#160;<span class="comment"> * \details Virtual data objects allow users to connect various nodes within a</span></div>
<div class="line"><a name="l00221"></a><span class="lineno">  221</span>&#160;<span class="comment"> * graph via data references without access to that data, but they also permit the</span></div>
<div class="line"><a name="l00222"></a><span class="lineno">  222</span>&#160;<span class="comment"> * implementation to take maximum advantage of possible optimizations. Use this</span></div>
<div class="line"><a name="l00223"></a><span class="lineno">  223</span>&#160;<span class="comment"> * API to create a data reference to link two or more nodes together when the</span></div>
<div class="line"><a name="l00224"></a><span class="lineno">  224</span>&#160;<span class="comment"> * intermediate data are not required to be accessed by outside entities. This API</span></div>
<div class="line"><a name="l00225"></a><span class="lineno">  225</span>&#160;<span class="comment"> * in particular allows the user to define the tensor data format of the data without</span></div>
<div class="line"><a name="l00226"></a><span class="lineno">  226</span>&#160;<span class="comment"> * requiring the exact dimensions. Virtual objects are scoped within the graph</span></div>
<div class="line"><a name="l00227"></a><span class="lineno">  227</span>&#160;<span class="comment"> * they are declared a part of, and can&#39;t be shared outside of this scope.</span></div>
<div class="line"><a name="l00228"></a><span class="lineno">  228</span>&#160;<span class="comment"> * \param [in] graph The reference to the parent graph.</span></div>
<div class="line"><a name="l00229"></a><span class="lineno">  229</span>&#160;<span class="comment"> * \param [in] num_of_dims The number of dimensions.</span></div>
<div class="line"><a name="l00230"></a><span class="lineno">  230</span>&#160;<span class="comment"> * \param [in] sizes Dimensions sizes in elements.</span></div>
<div class="line"><a name="l00231"></a><span class="lineno">  231</span>&#160;<span class="comment"> * \param [in] data_format The &lt;tt&gt;vx_type_t&lt;/tt&gt; that represents the data type of the tensor data elements.</span></div>
<div class="line"><a name="l00232"></a><span class="lineno">  232</span>&#160;<span class="comment"> * \param [in] fixed_point_pos Specifies the fixed point position when the input element type is int16, if 0 calculations are performed in integer math</span></div>
<div class="line"><a name="l00233"></a><span class="lineno">  233</span>&#160;<span class="comment"> * \return A tensor data reference or zero when an error is encountered.</span></div>
<div class="line"><a name="l00234"></a><span class="lineno">  234</span>&#160;<span class="comment"> * \note Passing this reference to &lt;tt&gt;\ref vxCopyTensorPatch&lt;/tt&gt; will return an error.</span></div>
<div class="line"><a name="l00235"></a><span class="lineno">  235</span>&#160;<span class="comment"> * \ingroup group_tensor</span></div>
<div class="line"><a name="l00236"></a><span class="lineno">  236</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00237"></a><span class="lineno">  237</span>&#160;VX_API_ENTRY <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#gad7451299a63894cd952324c9c073609c">vxCreateVirtualTensor</a>(vx_graph graph, vx_uint32 num_of_dims, vx_uint32 *sizes, vx_enum data_format, vx_uint8 fixed_point_pos);</div>
<div class="line"><a name="l00238"></a><span class="lineno">  238</span>&#160;</div>
<div class="line"><a name="l00239"></a><span class="lineno">  239</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00240"></a><span class="lineno">  240</span>&#160;<span class="comment">/*! \brief Allows the application to copy a view patch from/into an tensor object .</span></div>
<div class="line"><a name="l00241"></a><span class="lineno">  241</span>&#160;<span class="comment">* \param [in] tensor The reference to the tensor object that is the source or the</span></div>
<div class="line"><a name="l00242"></a><span class="lineno">  242</span>&#160;<span class="comment">* destination of the copy.</span></div>
<div class="line"><a name="l00243"></a><span class="lineno">  243</span>&#160;<span class="comment">* \param [in] view Optional parameter of type &lt;tt&gt;\ref vx_tensor_view&lt;/tt&gt;. The coordinates of the view patch. The patch must be within</span></div>
<div class="line"><a name="l00244"></a><span class="lineno">  244</span>&#160;<span class="comment">* the bounds of the tensor. (start[index],end[index]) gives the coordinates of the view</span></div>
<div class="line"><a name="l00245"></a><span class="lineno">  245</span>&#160;<span class="comment">* element out of the patch. Must be 0 &lt;= start &lt; end &lt;= number of elements in the tensor dimension.</span></div>
<div class="line"><a name="l00246"></a><span class="lineno">  246</span>&#160;<span class="comment">* see &lt;tt&gt;\ref vxCreateTensorView&lt;/tt&gt;. If NULL is given instead of the object. Then the function behaves as if view was the size of the full tensor.</span></div>
<div class="line"><a name="l00247"></a><span class="lineno">  247</span>&#160;<span class="comment">* \param [in] user_addr The address of a structure describing the layout of the</span></div>
<div class="line"><a name="l00248"></a><span class="lineno">  248</span>&#160;<span class="comment">* user memory location pointed by user_ptr. In the structure, dim[index],</span></div>
<div class="line"><a name="l00249"></a><span class="lineno">  249</span>&#160;<span class="comment">* stride[index] fields must be provided, other fields are ignored by the function.</span></div>
<div class="line"><a name="l00250"></a><span class="lineno">  250</span>&#160;<span class="comment">* The layout of the user memory must follow a row major order. see &lt;tt&gt;\ref vxCreateTensorAddressing&lt;/tt&gt;</span></div>
<div class="line"><a name="l00251"></a><span class="lineno">  251</span>&#160;<span class="comment">* \param [in] user_ptr The address of the memory location where to store the requested data</span></div>
<div class="line"><a name="l00252"></a><span class="lineno">  252</span>&#160;<span class="comment">* if the copy was requested in read mode, or from where to get the data to store into the tensor</span></div>
<div class="line"><a name="l00253"></a><span class="lineno">  253</span>&#160;<span class="comment">* object if the copy was requested in write mode. The accessible memory must be large enough</span></div>
<div class="line"><a name="l00254"></a><span class="lineno">  254</span>&#160;<span class="comment">* to contain the specified patch with the specified layout:\n</span></div>
<div class="line"><a name="l00255"></a><span class="lineno">  255</span>&#160;<span class="comment">* accessible memory in bytes &gt;= (end[last_dimension] - start[last_dimension]) * stride[last_dimension].\m</span></div>
<div class="line"><a name="l00256"></a><span class="lineno">  256</span>&#160;<span class="comment">* see &lt;tt&gt;\ref vxCreateTensorAddressing&lt;/tt&gt; and &lt;tt&gt;\ref vxCreateTensorView&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00257"></a><span class="lineno">  257</span>&#160;<span class="comment">* \param [in] usage This declares the effect of the copy with regard to the tensor object</span></div>
<div class="line"><a name="l00258"></a><span class="lineno">  258</span>&#160;<span class="comment">* using the &lt;tt&gt;vx_accessor_e&lt;/tt&gt; enumeration. Only VX_READ_ONLY and VX_WRITE_ONLY are supported:</span></div>
<div class="line"><a name="l00259"></a><span class="lineno">  259</span>&#160;<span class="comment">* \arg VX_READ_ONLY means that data is copied from the tensor object into the application memory</span></div>
<div class="line"><a name="l00260"></a><span class="lineno">  260</span>&#160;<span class="comment">* \arg VX_WRITE_ONLY means that data is copied into the tensor object from the application memory</span></div>
<div class="line"><a name="l00261"></a><span class="lineno">  261</span>&#160;<span class="comment">* \param [in] user_mem_type A &lt;tt&gt;vx_memory_type_e&lt;/tt&gt; enumeration that specifies</span></div>
<div class="line"><a name="l00262"></a><span class="lineno">  262</span>&#160;<span class="comment">* the memory type of the memory referenced by the user_addr.</span></div>
<div class="line"><a name="l00263"></a><span class="lineno">  263</span>&#160;<span class="comment">* \return A &lt;tt&gt;vx_status_e&lt;/tt&gt; enumeration.</span></div>
<div class="line"><a name="l00264"></a><span class="lineno">  264</span>&#160;<span class="comment">* \retval VX_ERROR_OPTIMIZED_AWAY This is a reference to a virtual tensor that cannot be</span></div>
<div class="line"><a name="l00265"></a><span class="lineno">  265</span>&#160;<span class="comment">* accessed by the application.</span></div>
<div class="line"><a name="l00266"></a><span class="lineno">  266</span>&#160;<span class="comment">* \retval VX_ERROR_INVALID_REFERENCE The tensor reference is not actually an tensor reference.</span></div>
<div class="line"><a name="l00267"></a><span class="lineno">  267</span>&#160;<span class="comment">* \retval VX_ERROR_INVALID_PARAMETERS An other parameter is incorrect.</span></div>
<div class="line"><a name="l00268"></a><span class="lineno">  268</span>&#160;<span class="comment"> * \ingroup group_tensor</span></div>
<div class="line"><a name="l00269"></a><span class="lineno">  269</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00270"></a><span class="lineno">  270</span>&#160;VX_API_ENTRY vx_status VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#gabacda16928cb3ab9714402c64a9f9710">vxCopyTensorPatch</a>(<a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> tensor, <a class="code" href="../../dd/dad/group__group__tensor.html#ga372396aa9489f7fe30f9af0384d18340">vx_tensor_view</a> view, <a class="code" href="../../dd/dad/group__group__tensor.html#ga6f6c0db1e3f276331707a50522294e51">vx_tensor_addressing</a> user_addr, <span class="keywordtype">void</span> *user_ptr, vx_enum usage, vx_enum user_mem_type);</div>
<div class="line"><a name="l00271"></a><span class="lineno">  271</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00272"></a><span class="lineno">  272</span>&#160;<span class="comment">/*! \brief Retrieves various attributes of a tensor data.</span></div>
<div class="line"><a name="l00273"></a><span class="lineno">  273</span>&#160;<span class="comment"> * \param [in] tensor The reference to the tensor data to query.</span></div>
<div class="line"><a name="l00274"></a><span class="lineno">  274</span>&#160;<span class="comment"> * \param [in] attribute The attribute to query. Use a &lt;tt&gt;\ref vx_tensor_attribute_e&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00275"></a><span class="lineno">  275</span>&#160;<span class="comment"> * \param [out] ptr The location at which to store the resulting value.</span></div>
<div class="line"><a name="l00276"></a><span class="lineno">  276</span>&#160;<span class="comment"> * \param [in] size The size of the container to which \a ptr points.</span></div>
<div class="line"><a name="l00277"></a><span class="lineno">  277</span>&#160;<span class="comment"> * \return A &lt;tt&gt;vx_status_e&lt;/tt&gt; enumeration.</span></div>
<div class="line"><a name="l00278"></a><span class="lineno">  278</span>&#160;<span class="comment"> * \retval VX_SUCCESS No errors.</span></div>
<div class="line"><a name="l00279"></a><span class="lineno">  279</span>&#160;<span class="comment"> * \retval VX_ERROR_INVALID_REFERENCE If data is not a &lt;tt&gt;\ref vx_tensor&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00280"></a><span class="lineno">  280</span>&#160;<span class="comment"> * \retval VX_ERROR_INVALID_PARAMETERS If any of the other parameters are incorrect.</span></div>
<div class="line"><a name="l00281"></a><span class="lineno">  281</span>&#160;<span class="comment"> * \ingroup group_tensor</span></div>
<div class="line"><a name="l00282"></a><span class="lineno">  282</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00283"></a><span class="lineno">  283</span>&#160;VX_API_ENTRY vx_status VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#gaa96e480cdfdcb642d0f3f944ea6fec4f">vxQueryTensor</a>(<a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> tensor, vx_enum attribute, <span class="keywordtype">void</span> *ptr, vx_size size);</div>
<div class="line"><a name="l00284"></a><span class="lineno">  284</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00285"></a><span class="lineno">  285</span>&#160;<span class="comment">/*! \brief Releases a reference to a tensor data object.</span></div>
<div class="line"><a name="l00286"></a><span class="lineno">  286</span>&#160;<span class="comment"> * The object may not be garbage collected until its total reference count is zero.</span></div>
<div class="line"><a name="l00287"></a><span class="lineno">  287</span>&#160;<span class="comment"> * \param [in] tensor The pointer to the tensor data to release.</span></div>
<div class="line"><a name="l00288"></a><span class="lineno">  288</span>&#160;<span class="comment"> * \post After returning from this function the reference is zeroed.</span></div>
<div class="line"><a name="l00289"></a><span class="lineno">  289</span>&#160;<span class="comment"> * \return A &lt;tt&gt;vx_status_e&lt;/tt&gt; enumeration.</span></div>
<div class="line"><a name="l00290"></a><span class="lineno">  290</span>&#160;<span class="comment"> * \retval VX_SUCCESS No errors.</span></div>
<div class="line"><a name="l00291"></a><span class="lineno">  291</span>&#160;<span class="comment"> * \retval VX_SUCCESS Success</span></div>
<div class="line"><a name="l00292"></a><span class="lineno">  292</span>&#160;<span class="comment"> * \retval * An error occurred. See &lt;tt&gt;vx_status_e&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00293"></a><span class="lineno">  293</span>&#160;<span class="comment"> * \ingroup group_tensor</span></div>
<div class="line"><a name="l00294"></a><span class="lineno">  294</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00295"></a><span class="lineno">  295</span>&#160;VX_API_ENTRY vx_status VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#gafbdcb72a17b7d740db68ae9464d5d6fd">vxReleaseTensor</a>(<a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> *tensor);</div>
<div class="line"><a name="l00296"></a><span class="lineno">  296</span>&#160;</div>
<div class="line"><a name="l00297"></a><span class="lineno">  297</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00298"></a><span class="lineno">  298</span>&#160;<span class="comment">/*! \brief Create  an opaque reference to a tensor view object.</span></div>
<div class="line"><a name="l00299"></a><span class="lineno">  299</span>&#160;<span class="comment"> * \details Not guaranteed to exist until the &lt;tt&gt;vx_graph&lt;/tt&gt; containing it has been verified.</span></div>
<div class="line"><a name="l00300"></a><span class="lineno">  300</span>&#160;<span class="comment"> * \param [in] context The reference to the implementation context.</span></div>
<div class="line"><a name="l00301"></a><span class="lineno">  301</span>&#160;<span class="comment"> * \param [in] view_array_start a vx_uint32 array of start values of the view.</span></div>
<div class="line"><a name="l00302"></a><span class="lineno">  302</span>&#160;<span class="comment"> * \param [in] view_array_end a vx_uint32 array of end values of the view.</span></div>
<div class="line"><a name="l00303"></a><span class="lineno">  303</span>&#160;<span class="comment"> * \param [in] numViewDimensions number of dimensions of view_array_start and view_array_end.</span></div>
<div class="line"><a name="l00304"></a><span class="lineno">  304</span>&#160;<span class="comment"> * \return A tensor data view reference or zero when an error is encountered.</span></div>
<div class="line"><a name="l00305"></a><span class="lineno">  305</span>&#160;<span class="comment"> * \ingroup group_tensor</span></div>
<div class="line"><a name="l00306"></a><span class="lineno">  306</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00307"></a><span class="lineno">  307</span>&#160;VX_API_ENTRY <a class="code" href="../../dd/dad/group__group__tensor.html#ga372396aa9489f7fe30f9af0384d18340">vx_tensor_view</a> VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#ga2aaa8f0282d709a2bc3067737390ff5a">vxCreateTensorView</a>(vx_context context, vx_uint32 *view_array_start, vx_uint32 * view_array_end, vx_uint8 numViewDimensions);</div>
<div class="line"><a name="l00308"></a><span class="lineno">  308</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00309"></a><span class="lineno">  309</span>&#160;<span class="comment">/*! \brief Releases a reference to a tensor data view object.</span></div>
<div class="line"><a name="l00310"></a><span class="lineno">  310</span>&#160;<span class="comment">* The object may not be garbage collected until its total reference count is zero.</span></div>
<div class="line"><a name="l00311"></a><span class="lineno">  311</span>&#160;<span class="comment">* \param [in] tensor_view The pointer to the tensor data view to release.</span></div>
<div class="line"><a name="l00312"></a><span class="lineno">  312</span>&#160;<span class="comment">* \post After returning from this function the reference is zeroed.</span></div>
<div class="line"><a name="l00313"></a><span class="lineno">  313</span>&#160;<span class="comment">* \return A &lt;tt&gt;vx_status_e&lt;/tt&gt; enumeration.</span></div>
<div class="line"><a name="l00314"></a><span class="lineno">  314</span>&#160;<span class="comment">* \retval VX_SUCCESS No errors.</span></div>
<div class="line"><a name="l00315"></a><span class="lineno">  315</span>&#160;<span class="comment">* \retval VX_SUCCESS Success</span></div>
<div class="line"><a name="l00316"></a><span class="lineno">  316</span>&#160;<span class="comment">* \retval * An error occurred. See &lt;tt&gt;vx_status_e&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00317"></a><span class="lineno">  317</span>&#160;<span class="comment">* \ingroup group_tensor</span></div>
<div class="line"><a name="l00318"></a><span class="lineno">  318</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00319"></a><span class="lineno">  319</span>&#160;VX_API_ENTRY vx_status VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#ga4ec0834c08f2a94465891d3ad440729d">vxReleaseTensorView</a>(<a class="code" href="../../dd/dad/group__group__tensor.html#ga372396aa9489f7fe30f9af0384d18340">vx_tensor_view</a> *tensor_view);</div>
<div class="line"><a name="l00320"></a><span class="lineno">  320</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00321"></a><span class="lineno">  321</span>&#160;<span class="comment">/*! \brief Create  an opaque reference to a tensor addressing object.</span></div>
<div class="line"><a name="l00322"></a><span class="lineno">  322</span>&#160;<span class="comment">* \details Not guaranteed to exist until the &lt;tt&gt;vx_graph&lt;/tt&gt; containing it has been verified.</span></div>
<div class="line"><a name="l00323"></a><span class="lineno">  323</span>&#160;<span class="comment">* \param [in] context The reference to the implementation context.</span></div>
<div class="line"><a name="l00324"></a><span class="lineno">  324</span>&#160;<span class="comment">* \param [in] addressing_array_dimension a vx_uint32 array of sLength of patch in all dimensions in elements.</span></div>
<div class="line"><a name="l00325"></a><span class="lineno">  325</span>&#160;<span class="comment">* \param [in] addressing_array_stride a vx_uint32 arrayStride in all dimensions in bytes.</span></div>
<div class="line"><a name="l00326"></a><span class="lineno">  326</span>&#160;<span class="comment">* \param [in] numViewDimensions number of dimensions of view_array_start and view_array_end.</span></div>
<div class="line"><a name="l00327"></a><span class="lineno">  327</span>&#160;<span class="comment">* \return A tensor data view reference or zero when an error is encountered.</span></div>
<div class="line"><a name="l00328"></a><span class="lineno">  328</span>&#160;<span class="comment">* \ingroup group_tensor</span></div>
<div class="line"><a name="l00329"></a><span class="lineno">  329</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00330"></a><span class="lineno">  330</span>&#160;VX_API_ENTRY <a class="code" href="../../dd/dad/group__group__tensor.html#ga6f6c0db1e3f276331707a50522294e51">vx_tensor_addressing</a> VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#gad7487061cf90c8b1b1e2538bf79d3eaf">vxCreateTensorAddressing</a>(vx_context context, vx_uint32 *addressing_array_dimension, vx_uint32 * addressing_array_stride, vx_uint8 numViewDimensions);</div>
<div class="line"><a name="l00331"></a><span class="lineno">  331</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00332"></a><span class="lineno">  332</span>&#160;<span class="comment">/*! \brief Releases a reference to a tensor data addressing object.</span></div>
<div class="line"><a name="l00333"></a><span class="lineno">  333</span>&#160;<span class="comment">* The object may not be garbage collected until its total reference count is zero.</span></div>
<div class="line"><a name="l00334"></a><span class="lineno">  334</span>&#160;<span class="comment">* \param [in] tensor_addr The pointer to the tensor data addressing to release.</span></div>
<div class="line"><a name="l00335"></a><span class="lineno">  335</span>&#160;<span class="comment">* \post After returning from this function the reference is zeroed.</span></div>
<div class="line"><a name="l00336"></a><span class="lineno">  336</span>&#160;<span class="comment">* \return A &lt;tt&gt;vx_status_e&lt;/tt&gt; enumeration.</span></div>
<div class="line"><a name="l00337"></a><span class="lineno">  337</span>&#160;<span class="comment">* \retval VX_SUCCESS No errors.</span></div>
<div class="line"><a name="l00338"></a><span class="lineno">  338</span>&#160;<span class="comment">* \retval VX_SUCCESS Success</span></div>
<div class="line"><a name="l00339"></a><span class="lineno">  339</span>&#160;<span class="comment">* \retval * An error occurred. See &lt;tt&gt;vx_status_e&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00340"></a><span class="lineno">  340</span>&#160;<span class="comment">* \ingroup group_tensor</span></div>
<div class="line"><a name="l00341"></a><span class="lineno">  341</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00342"></a><span class="lineno">  342</span>&#160;VX_API_ENTRY vx_status VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#ga30b8f5de5443a53f65746ea5dd0050c9">vxReleaseTensorAddressing</a>(<a class="code" href="../../dd/dad/group__group__tensor.html#ga6f6c0db1e3f276331707a50522294e51">vx_tensor_addressing</a> *tensor_addr);</div>
<div class="line"><a name="l00343"></a><span class="lineno">  343</span>&#160;</div>
<div class="line"><a name="l00344"></a><span class="lineno">  344</span>&#160;<span class="comment">/*==============================================================================</span></div>
<div class="line"><a name="l00345"></a><span class="lineno">  345</span>&#160;<span class="comment">    NN Nodes</span></div>
<div class="line"><a name="l00346"></a><span class="lineno">  346</span>&#160;<span class="comment">=============================================================================*/</span><span class="comment"></span></div>
<div class="line"><a name="l00347"></a><span class="lineno">  347</span>&#160;<span class="comment">/*! \brief [Graph] Creates a Convolutional Network Convolution Layer Node.</span></div>
<div class="line"><a name="l00348"></a><span class="lineno">  348</span>&#160;<span class="comment">* \details This function implement Convolutional Network Convolution layer.</span></div>
<div class="line"><a name="l00349"></a><span class="lineno">  349</span>&#160;<span class="comment">* In case the input and output &lt;tt&gt;\ref vx_tensor&lt;/tt&gt; are signed 16. A fixed point calculation is performed with round and saturate according to the number of accumulator bits. \n</span></div>
<div class="line"><a name="l00350"></a><span class="lineno">  350</span>&#160;<span class="comment">* round: rounding according the &lt;tt&gt;vx_round_policy_e&lt;/tt&gt; enumeration. \n</span></div>
<div class="line"><a name="l00351"></a><span class="lineno">  351</span>&#160;<span class="comment">* saturate: A saturation according the &lt;tt&gt;vx_convert_policy_e&lt;/tt&gt; enumeration.</span></div>
<div class="line"><a name="l00352"></a><span class="lineno">  352</span>&#160;<span class="comment">* The saturation is done based on the accumulator_bits parameter.</span></div>
<div class="line"><a name="l00353"></a><span class="lineno">  353</span>&#160;<span class="comment">* According the accumulator_bits, the saturation might not be performed every operation. </span></div>
<div class="line"><a name="l00354"></a><span class="lineno">  354</span>&#160;<span class="comment">* But every a specified amount of operations, </span></div>
<div class="line"><a name="l00355"></a><span class="lineno">  355</span>&#160;<span class="comment">* that are suspected to saturate the accumulation bits\n</span></div>
<div class="line"><a name="l00356"></a><span class="lineno">  356</span>&#160;<span class="comment">* The following equation is implemented: \n</span></div>
<div class="line"><a name="l00357"></a><span class="lineno">  357</span>&#160;<span class="comment">* \f$ outputs[j,k,i] = (\sum_{l} \sum_{m,n} saturate(round(inputs[j-m,k-n,l] \times weights[m,n,l,i])))+biasses[j,k,i] \f$\n</span></div>
<div class="line"><a name="l00358"></a><span class="lineno">  358</span>&#160;<span class="comment">* Where \f$m,n\f$ are indexes on the convolution matrices. \f$ l\f$ is an index on all the convolutions per input.\f$ i\f$ is an index per output.</span></div>
<div class="line"><a name="l00359"></a><span class="lineno">  359</span>&#160;<span class="comment">* \f$ j,k \f$ are the inputs/outputs spatial indexes.</span></div>
<div class="line"><a name="l00360"></a><span class="lineno">  360</span>&#160;<span class="comment">* Convolution is done on the first 2 dimensions of the &lt;tt&gt;\ref vx_tensor&lt;/tt&gt;. Therefore, we use here the term x for the first dimension and y for the second.\n</span></div>
<div class="line"><a name="l00361"></a><span class="lineno">  361</span>&#160;<span class="comment">* before the Convolution is done, a padding of the first 2D with zeros is performed.</span></div>
<div class="line"><a name="l00362"></a><span class="lineno">  362</span>&#160;<span class="comment">* Then down scale is done by picking the results according to a skip jump. The skip in the x and y dimension is determined by the output size dimensions.</span></div>
<div class="line"><a name="l00363"></a><span class="lineno">  363</span>&#160;<span class="comment">* The relation between input to output is as follows: \n</span></div>
<div class="line"><a name="l00364"></a><span class="lineno">  364</span>&#160;<span class="comment">* \f$ width_{output} = round(\frac{(width + 2 * pad_x - kernel_x)}{skip_x} + 1) \f$\n</span></div>
<div class="line"><a name="l00365"></a><span class="lineno">  365</span>&#160;<span class="comment">* and \n </span></div>
<div class="line"><a name="l00366"></a><span class="lineno">  366</span>&#160;<span class="comment">* \f$ height_{output} = round(\frac{(height + 2 * pad_y - kernel_y)}{skip_y} + 1) \f$\n</span></div>
<div class="line"><a name="l00367"></a><span class="lineno">  367</span>&#160;<span class="comment">* where \f$width\f$ is the size of the first input dimension. \f$height\f$ is the size of the second input dimension.</span></div>
<div class="line"><a name="l00368"></a><span class="lineno">  368</span>&#160;<span class="comment">* \f$width_{output}\f$ is the size of the first output dimension. \f$height_{output}\f$ is the size of the second output dimension.</span></div>
<div class="line"><a name="l00369"></a><span class="lineno">  369</span>&#160;<span class="comment">* \f$kernel_x\f$ and \f$kernel_y\f$ are the convolution sizes in x and y.</span></div>
<div class="line"><a name="l00370"></a><span class="lineno">  370</span>&#160;<span class="comment">* skip is calculated by the relation between input and output.</span></div>
<div class="line"><a name="l00371"></a><span class="lineno">  371</span>&#160;<span class="comment">* rounding is done according to &lt;tt&gt;\ref vx_convolutional_network_rounding_type_e&lt;/tt&gt;. </span></div>
<div class="line"><a name="l00372"></a><span class="lineno">  372</span>&#160;<span class="comment">* \param [in] graph The handle to the graph.</span></div>
<div class="line"><a name="l00373"></a><span class="lineno">  373</span>&#160;<span class="comment">* \param [in] inputs The input tensor data. 3 lower dims represent a single input, and an optional 4th dimension for batch of inputs.\n</span></div>
<div class="line"><a name="l00374"></a><span class="lineno">  374</span>&#160;<span class="comment">* \param [in] weights Weights are 4d tensor with dimensions [kernel_x, kernel_y, #IFM, #OFM].\n</span></div>
<div class="line"><a name="l00375"></a><span class="lineno">  375</span>&#160;<span class="comment">* \param [in] biases The biases, which may be shared (one per ofm) or unshared (one per ofm * output location).</span></div>
<div class="line"><a name="l00376"></a><span class="lineno">  376</span>&#160;<span class="comment">* \param [in] pad_x Number of elements added at each side in the x dimension of the input.</span></div>
<div class="line"><a name="l00377"></a><span class="lineno">  377</span>&#160;<span class="comment">* \param [in] pad_y Number of elements added at each side in the y dimension of the input. In fully connected layers this input is ignored.</span></div>
<div class="line"><a name="l00378"></a><span class="lineno">  378</span>&#160;<span class="comment">* \param [in] accumulator_bits Is the total number of bits used during intermediate accumulation.</span></div>
<div class="line"><a name="l00379"></a><span class="lineno">  379</span>&#160;<span class="comment">* \param [in] overflow_policy A &lt;tt&gt; VX_TYPE_ENUM&lt;/tt&gt; of the &lt;tt&gt; vx_convert_policy_e&lt;/tt&gt; enumeration.</span></div>
<div class="line"><a name="l00380"></a><span class="lineno">  380</span>&#160;<span class="comment">* \param [in] rounding_policy A &lt;tt&gt; VX_TYPE_ENUM&lt;/tt&gt; of the &lt;tt&gt; vx_round_policy_e&lt;/tt&gt; enumeration.</span></div>
<div class="line"><a name="l00381"></a><span class="lineno">  381</span>&#160;<span class="comment">* \param [in] down_scale_size_rounding Rounding method for calculating output dimensions. See &lt;tt&gt;\ref vx_convolutional_network_rounding_type_e&lt;/tt&gt;</span></div>
<div class="line"><a name="l00382"></a><span class="lineno">  382</span>&#160;<span class="comment">* \param [out] outputs The output tensor data. Output will have the same number of dimensions as input.</span></div>
<div class="line"><a name="l00383"></a><span class="lineno">  383</span>&#160;<span class="comment">* \return &lt;tt&gt; vx_node&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00384"></a><span class="lineno">  384</span>&#160;<span class="comment">* \retval 0 Node could not be created.</span></div>
<div class="line"><a name="l00385"></a><span class="lineno">  385</span>&#160;<span class="comment">* \retval * Node handle.</span></div>
<div class="line"><a name="l00386"></a><span class="lineno">  386</span>&#160;<span class="comment">* \ingroup group_cnn</span></div>
<div class="line"><a name="l00387"></a><span class="lineno">  387</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00388"></a><span class="lineno">  388</span>&#160;VX_API_ENTRY vx_node VX_API_CALL <a class="code" href="../../d6/d9a/group__group__cnn.html#ga870c106e8ceb4c118692c6f754f75f43">vxConvolutionLayer</a>(vx_graph graph, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> inputs, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> weights, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> biases,</div>
<div class="line"><a name="l00389"></a><span class="lineno">  389</span>&#160;    vx_uint32 pad_x,</div>
<div class="line"><a name="l00390"></a><span class="lineno">  390</span>&#160;    vx_uint32 pad_y,</div>
<div class="line"><a name="l00391"></a><span class="lineno">  391</span>&#160;    vx_uint8 accumulator_bits, </div>
<div class="line"><a name="l00392"></a><span class="lineno">  392</span>&#160;    vx_enum overflow_policy, </div>
<div class="line"><a name="l00393"></a><span class="lineno">  393</span>&#160;    vx_enum rounding_policy,</div>
<div class="line"><a name="l00394"></a><span class="lineno">  394</span>&#160;    vx_enum down_scale_size_rounding,   </div>
<div class="line"><a name="l00395"></a><span class="lineno">  395</span>&#160;    <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> outputs);</div>
<div class="line"><a name="l00396"></a><span class="lineno">  396</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00397"></a><span class="lineno">  397</span>&#160;<span class="comment">/*! \brief [Graph] Creates a Fully connected Convolutional Network Layer Node.</span></div>
<div class="line"><a name="l00398"></a><span class="lineno">  398</span>&#160;<span class="comment">* \details This function implement Fully connected Convolutional Network layers.</span></div>
<div class="line"><a name="l00399"></a><span class="lineno">  399</span>&#160;<span class="comment">* In case the input and output &lt;tt&gt;\ref vx_tensor&lt;/tt&gt; are signed 16. A fixed point calculation is performed with round and saturate according to the number of accumulator bits. \n</span></div>
<div class="line"><a name="l00400"></a><span class="lineno">  400</span>&#160;<span class="comment">* round: rounding according the &lt;tt&gt;vx_round_policy_e&lt;/tt&gt; enumeration. \n</span></div>
<div class="line"><a name="l00401"></a><span class="lineno">  401</span>&#160;<span class="comment">* saturate: A saturation according the &lt;tt&gt;vx_convert_policy_e&lt;/tt&gt; enumeration.</span></div>
<div class="line"><a name="l00402"></a><span class="lineno">  402</span>&#160;<span class="comment">* The saturation is done based on the accumulator_bits parameter.</span></div>
<div class="line"><a name="l00403"></a><span class="lineno">  403</span>&#160;<span class="comment">* According the accumulator_bits, the saturation might not be performed every operation. </span></div>
<div class="line"><a name="l00404"></a><span class="lineno">  404</span>&#160;<span class="comment">* But every a specified amount of operations, </span></div>
<div class="line"><a name="l00405"></a><span class="lineno">  405</span>&#160;<span class="comment">* that are suspected to saturate the accumulation bits\n</span></div>
<div class="line"><a name="l00406"></a><span class="lineno">  406</span>&#160;<span class="comment">* The equation for Fully connected layer:\n</span></div>
<div class="line"><a name="l00407"></a><span class="lineno">  407</span>&#160;<span class="comment">* \f$ outputs[i] = ( \sum_{j} saturate(round(inputs[j] \times weights[j,i])))+biasses[i] \f$\n</span></div>
<div class="line"><a name="l00408"></a><span class="lineno">  408</span>&#160;<span class="comment">* Where \f$j\f$ is a index on the input feature and \f$i\f$ is a index on the output.</span></div>
<div class="line"><a name="l00409"></a><span class="lineno">  409</span>&#160;<span class="comment">* before the fully connected is done, a padding of the input is performed.</span></div>
<div class="line"><a name="l00410"></a><span class="lineno">  410</span>&#160;<span class="comment">* Then down scale is done by picking the results according to a skip jump. The skip is determined by the output size dimensions.</span></div>
<div class="line"><a name="l00411"></a><span class="lineno">  411</span>&#160;<span class="comment">* The relation between input to output is as follows:</span></div>
<div class="line"><a name="l00412"></a><span class="lineno">  412</span>&#160;<span class="comment">* \f$ size_{output} = round(\frac{(size_{input} + 2 * pad)}{skip} + 1) \f$\n</span></div>
<div class="line"><a name="l00413"></a><span class="lineno">  413</span>&#160;<span class="comment">* where \f$size_{input}\f$ is the size of the input dimension. </span></div>
<div class="line"><a name="l00414"></a><span class="lineno">  414</span>&#160;<span class="comment">* \f$size_{output}\f$ is the size of the output dimension. </span></div>
<div class="line"><a name="l00415"></a><span class="lineno">  415</span>&#160;<span class="comment">* skip is calculated by the relation between input and output.</span></div>
<div class="line"><a name="l00416"></a><span class="lineno">  416</span>&#160;<span class="comment">* rounding is done according to &lt;tt&gt;\ref vx_convolutional_network_rounding_type_e&lt;/tt&gt;. </span></div>
<div class="line"><a name="l00417"></a><span class="lineno">  417</span>&#160;<span class="comment">* \param [in] graph The handle to the graph.</span></div>
<div class="line"><a name="l00418"></a><span class="lineno">  418</span>&#160;<span class="comment">* \param [in] inputs The input tensor data. 1-3 lower dims represent a single input, and all dims above dim(weights)-1 are optional for batch of inputs. Note that batch may be multidimensional.</span></div>
<div class="line"><a name="l00419"></a><span class="lineno">  419</span>&#160;<span class="comment">* \param [in] weights Number of dimensions equals dim(single input)+1. Single input dims are [width, height, #IFM], with height and #IFM being optional.\n</span></div>
<div class="line"><a name="l00420"></a><span class="lineno">  420</span>&#160;<span class="comment">* \param [in] biases The biases, which may be shared (one per ofm) or unshared (one per ofm * output location).</span></div>
<div class="line"><a name="l00421"></a><span class="lineno">  421</span>&#160;<span class="comment">* \param [in] pad Number of elements added at each side in the input.</span></div>
<div class="line"><a name="l00422"></a><span class="lineno">  422</span>&#160;<span class="comment">* \param [in] accumulator_bits Is the total number of bits used during intermediate accumulation.</span></div>
<div class="line"><a name="l00423"></a><span class="lineno">  423</span>&#160;<span class="comment">* \param [in] overflow_policy A &lt;tt&gt; VX_TYPE_ENUM&lt;/tt&gt; of the &lt;tt&gt; vx_convert_policy_e&lt;/tt&gt; enumeration.</span></div>
<div class="line"><a name="l00424"></a><span class="lineno">  424</span>&#160;<span class="comment">* \param [in] rounding_policy A &lt;tt&gt; VX_TYPE_ENUM&lt;/tt&gt; of the &lt;tt&gt; vx_round_policy_e&lt;/tt&gt; enumeration.</span></div>
<div class="line"><a name="l00425"></a><span class="lineno">  425</span>&#160;<span class="comment">* \param [in] down_scale_size_rounding Rounding method for calculating output dimensions. See &lt;tt&gt;\ref vx_convolutional_network_rounding_type_e&lt;/tt&gt;</span></div>
<div class="line"><a name="l00426"></a><span class="lineno">  426</span>&#160;<span class="comment">* \param [out] outputs The output tensor data. Output will have the same number of dimensions as input.</span></div>
<div class="line"><a name="l00427"></a><span class="lineno">  427</span>&#160;<span class="comment">* \return &lt;tt&gt; vx_node&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00428"></a><span class="lineno">  428</span>&#160;<span class="comment">* \retval 0 Node could not be created.</span></div>
<div class="line"><a name="l00429"></a><span class="lineno">  429</span>&#160;<span class="comment">* \retval * Node handle.</span></div>
<div class="line"><a name="l00430"></a><span class="lineno">  430</span>&#160;<span class="comment">* \ingroup group_cnn</span></div>
<div class="line"><a name="l00431"></a><span class="lineno">  431</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00432"></a><span class="lineno">  432</span>&#160;VX_API_ENTRY vx_node VX_API_CALL <a class="code" href="../../d6/d9a/group__group__cnn.html#gaf940a27e357e2836af6a6492148f4c93">vxFullyConnectedLayer</a>(vx_graph graph, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> inputs, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> weights, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> biases,</div>
<div class="line"><a name="l00433"></a><span class="lineno">  433</span>&#160;    vx_uint32 pad,</div>
<div class="line"><a name="l00434"></a><span class="lineno">  434</span>&#160;    vx_uint8 accumulator_bits, </div>
<div class="line"><a name="l00435"></a><span class="lineno">  435</span>&#160;    vx_enum overflow_policy, </div>
<div class="line"><a name="l00436"></a><span class="lineno">  436</span>&#160;    vx_enum rounding_policy, </div>
<div class="line"><a name="l00437"></a><span class="lineno">  437</span>&#160;    vx_enum down_scale_size_rounding,   </div>
<div class="line"><a name="l00438"></a><span class="lineno">  438</span>&#160;    <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> outputs);</div>
<div class="line"><a name="l00439"></a><span class="lineno">  439</span>&#160;</div>
<div class="line"><a name="l00440"></a><span class="lineno">  440</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00441"></a><span class="lineno">  441</span>&#160;<span class="comment">/*! \brief [Graph] Creates a Convolutional Network Pooling Layer Node.</span></div>
<div class="line"><a name="l00442"></a><span class="lineno">  442</span>&#160;<span class="comment"> * \details Pooling is done on the first 2 dimensions or the &lt;tt&gt;\ref vx_tensor&lt;/tt&gt;. Therefore, we use here the term x for the first dimension and y for the second.\n</span></div>
<div class="line"><a name="l00443"></a><span class="lineno">  443</span>&#160;<span class="comment"> * Pooling operation is a function operation over a rectangle size and then a nearest neighbour down scale.</span></div>
<div class="line"><a name="l00444"></a><span class="lineno">  444</span>&#160;<span class="comment"> * Here we use pool_size_x and pool_size_y to specify the rectangle size on which the operation</span></div>
<div class="line"><a name="l00445"></a><span class="lineno">  445</span>&#160;<span class="comment"> * is performed. \n</span></div>
<div class="line"><a name="l00446"></a><span class="lineno">  446</span>&#160;<span class="comment"> * before the operation is done (average or maximum value). the data is padded in the first 2D with zeros.</span></div>
<div class="line"><a name="l00447"></a><span class="lineno">  447</span>&#160;<span class="comment"> * The down scale is done by picking the results according to a skip jump. The skip in the x and y dimension is determined by the output size dimensions.</span></div>
<div class="line"><a name="l00448"></a><span class="lineno">  448</span>&#160;<span class="comment">* \param [in] graph The handle to the graph.</span></div>
<div class="line"><a name="l00449"></a><span class="lineno">  449</span>&#160;<span class="comment">* \param [in] inputs The input tensor data. 3 lower dims represent a single input with dimensions [width, height, IFM], and an optional 4th dimension for batch of inputs.</span></div>
<div class="line"><a name="l00450"></a><span class="lineno">  450</span>&#160;<span class="comment">* \param [in] pool_type Either max pooling or average pooling (see &lt;tt&gt;\ref vx_convolutional_network_pooling_type_e&lt;/tt&gt;).</span></div>
<div class="line"><a name="l00451"></a><span class="lineno">  451</span>&#160;<span class="comment">* \param [in] pool_size_x Size of the pooling region in the x dimension</span></div>
<div class="line"><a name="l00452"></a><span class="lineno">  452</span>&#160;<span class="comment">* \param [in] pool_size_y Size of the pooling region in the y dimension. </span></div>
<div class="line"><a name="l00453"></a><span class="lineno">  453</span>&#160;<span class="comment">* \param [in] pool_pad_x Padding size in the x dimension. </span></div>
<div class="line"><a name="l00454"></a><span class="lineno">  454</span>&#160;<span class="comment">* \param [in] pool_pad_y Padding size in the y dimension.</span></div>
<div class="line"><a name="l00455"></a><span class="lineno">  455</span>&#160;<span class="comment">* \param [in] rounding, Rounding method for calculating output dimensions. See &lt;tt&gt;\ref vx_convolutional_network_rounding_type_e&lt;/tt&gt;</span></div>
<div class="line"><a name="l00456"></a><span class="lineno">  456</span>&#160;<span class="comment">* \param [out] outputs The output tensor data. Output will have the same number of dimensions as input.</span></div>
<div class="line"><a name="l00457"></a><span class="lineno">  457</span>&#160;<span class="comment">* \return &lt;tt&gt; vx_node&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00458"></a><span class="lineno">  458</span>&#160;<span class="comment">* \retval 0 Node could not be created.</span></div>
<div class="line"><a name="l00459"></a><span class="lineno">  459</span>&#160;<span class="comment">* \retval * Node handle.</span></div>
<div class="line"><a name="l00460"></a><span class="lineno">  460</span>&#160;<span class="comment">* \ingroup group_cnn</span></div>
<div class="line"><a name="l00461"></a><span class="lineno">  461</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00462"></a><span class="lineno">  462</span>&#160;VX_API_ENTRY vx_node VX_API_CALL <a class="code" href="../../d6/d9a/group__group__cnn.html#ga0b2cc8c5e172128a3014b56a1ae7b173">vxPoolingLayer</a>(vx_graph graph, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> inputs, vx_enum pool_type, </div>
<div class="line"><a name="l00463"></a><span class="lineno">  463</span>&#160;    vx_uint32 pool_size_x,</div>
<div class="line"><a name="l00464"></a><span class="lineno">  464</span>&#160;    vx_uint32 pool_size_y,</div>
<div class="line"><a name="l00465"></a><span class="lineno">  465</span>&#160;    vx_uint32 pool_pad_x,</div>
<div class="line"><a name="l00466"></a><span class="lineno">  466</span>&#160;    vx_uint32 pool_pad_y,</div>
<div class="line"><a name="l00467"></a><span class="lineno">  467</span>&#160;    vx_enum rounding,</div>
<div class="line"><a name="l00468"></a><span class="lineno">  468</span>&#160;    <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> outputs);</div>
<div class="line"><a name="l00469"></a><span class="lineno">  469</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00470"></a><span class="lineno">  470</span>&#160;<span class="comment">/*! \brief [Graph] Creates a Convolutional Network Softmax Layer Node.</span></div>
<div class="line"><a name="l00471"></a><span class="lineno">  471</span>&#160;<span class="comment">* \param [in] graph The handle to the graph.</span></div>
<div class="line"><a name="l00472"></a><span class="lineno">  472</span>&#160;<span class="comment">* \param [in] inputs The input tensor data, with number of dimensions equals dim(input batch) + 1. Softmax will be calculated per IFM.</span></div>
<div class="line"><a name="l00473"></a><span class="lineno">  473</span>&#160;<span class="comment">* \param [out] outputs The output tensor data. Output will have the same number of dimensions as input.</span></div>
<div class="line"><a name="l00474"></a><span class="lineno">  474</span>&#160;<span class="comment">* \ingroup group_cnn</span></div>
<div class="line"><a name="l00475"></a><span class="lineno">  475</span>&#160;<span class="comment">* \return &lt;tt&gt; vx_node&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00476"></a><span class="lineno">  476</span>&#160;<span class="comment">* \retval 0 Node could not be created.</span></div>
<div class="line"><a name="l00477"></a><span class="lineno">  477</span>&#160;<span class="comment">* \retval * Node handle.</span></div>
<div class="line"><a name="l00478"></a><span class="lineno">  478</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00479"></a><span class="lineno">  479</span>&#160;VX_API_ENTRY vx_node VX_API_CALL <a class="code" href="../../d6/d9a/group__group__cnn.html#ga76a3a94a73992017519d7dc01e81c476">vxSoftmaxLayer</a>(vx_graph graph, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> inputs, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> outputs);</div>
<div class="line"><a name="l00480"></a><span class="lineno">  480</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00481"></a><span class="lineno">  481</span>&#160;<span class="comment">/*! \brief [Graph] Creates a Convolutional Network Normalization Layer Node.</span></div>
<div class="line"><a name="l00482"></a><span class="lineno">  482</span>&#160;<span class="comment">* \details Normalizing over local input regions. Each input value is divided by \f$ (1+\frac{\alpha}{n}\sum_i x^2_i)^\beta \f$ , where n is the number of elements to normalize across.</span></div>
<div class="line"><a name="l00483"></a><span class="lineno">  483</span>&#160;<span class="comment">* and the sum is taken over the region centred at that value (zero padding is added where necessary).</span></div>
<div class="line"><a name="l00484"></a><span class="lineno">  484</span>&#160;<span class="comment">* \param [in] graph The handle to the graph.</span></div>
<div class="line"><a name="l00485"></a><span class="lineno">  485</span>&#160;<span class="comment">* \param [in] inputs The input tensor data. 3 lower dims represent a single input with dimensions [width, height, IFM], and an optional 4th dimension for batch of inputs.</span></div>
<div class="line"><a name="l00486"></a><span class="lineno">  486</span>&#160;<span class="comment">* \param [in] type Either same map or across maps (see vx_convolutional_network_norm_type_e).</span></div>
<div class="line"><a name="l00487"></a><span class="lineno">  487</span>&#160;<span class="comment">* \param [in] norm_size Number of elements to normalize across.</span></div>
<div class="line"><a name="l00488"></a><span class="lineno">  488</span>&#160;<span class="comment">* \param [in] alpha Alpha parameter in the normalization equation.</span></div>
<div class="line"><a name="l00489"></a><span class="lineno">  489</span>&#160;<span class="comment">* \param [in] beta  Beta parameter in the normalization equation.</span></div>
<div class="line"><a name="l00490"></a><span class="lineno">  490</span>&#160;<span class="comment">* \param [out] outputs The output tensor data. Output will have the same number of dimensions as input.</span></div>
<div class="line"><a name="l00491"></a><span class="lineno">  491</span>&#160;<span class="comment">* \ingroup group_cnn</span></div>
<div class="line"><a name="l00492"></a><span class="lineno">  492</span>&#160;<span class="comment">* \return &lt;tt&gt; vx_node&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00493"></a><span class="lineno">  493</span>&#160;<span class="comment">* \retval 0 Node could not be created.</span></div>
<div class="line"><a name="l00494"></a><span class="lineno">  494</span>&#160;<span class="comment">* \retval * Node handle.</span></div>
<div class="line"><a name="l00495"></a><span class="lineno">  495</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00496"></a><span class="lineno">  496</span>&#160;VX_API_ENTRY vx_node VX_API_CALL <a class="code" href="../../d6/d9a/group__group__cnn.html#ga2269eca24047c7b564ab7708a8420afa">vxNormalizationLayer</a>(vx_graph graph, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> inputs, vx_enum type,</div>
<div class="line"><a name="l00497"></a><span class="lineno">  497</span>&#160;    vx_uint32 norm_size,</div>
<div class="line"><a name="l00498"></a><span class="lineno">  498</span>&#160;    vx_float32 alpha,</div>
<div class="line"><a name="l00499"></a><span class="lineno">  499</span>&#160;    vx_float32 beta,</div>
<div class="line"><a name="l00500"></a><span class="lineno">  500</span>&#160;    <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> outputs);</div>
<div class="line"><a name="l00501"></a><span class="lineno">  501</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00502"></a><span class="lineno">  502</span>&#160;<span class="comment">/*! \brief [Graph] Creates a Convolutional Network Activation Layer Node.</span></div>
<div class="line"><a name="l00503"></a><span class="lineno">  503</span>&#160;<span class="comment">* \param [in] graph The handle to the graph.</span></div>
<div class="line"><a name="l00504"></a><span class="lineno">  504</span>&#160;<span class="comment">* \param [in] inputs The input tensor data.</span></div>
<div class="line"><a name="l00505"></a><span class="lineno">  505</span>&#160;<span class="comment">* \param [in] func Non-linear function (see &lt;tt&gt;\ref vx_convolutional_network_activation_func_e&lt;/tt&gt;).</span></div>
<div class="line"><a name="l00506"></a><span class="lineno">  506</span>&#160;<span class="comment">* \param [in] a Function parameters a. (see &lt;tt&gt;\ref vx_convolutional_network_activation_func_e&lt;/tt&gt;).</span></div>
<div class="line"><a name="l00507"></a><span class="lineno">  507</span>&#160;<span class="comment">* \param [in] b Function parameters b. (see &lt;tt&gt;\ref vx_convolutional_network_activation_func_e&lt;/tt&gt;).</span></div>
<div class="line"><a name="l00508"></a><span class="lineno">  508</span>&#160;<span class="comment">* \param [out] outputs The output tensor data. Output will have the same number of dimensions as input.</span></div>
<div class="line"><a name="l00509"></a><span class="lineno">  509</span>&#160;<span class="comment">* \ingroup group_cnn</span></div>
<div class="line"><a name="l00510"></a><span class="lineno">  510</span>&#160;<span class="comment">* \return &lt;tt&gt; vx_node&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00511"></a><span class="lineno">  511</span>&#160;<span class="comment">* \retval 0 Node could not be created.</span></div>
<div class="line"><a name="l00512"></a><span class="lineno">  512</span>&#160;<span class="comment">* \retval * Node handle.</span></div>
<div class="line"><a name="l00513"></a><span class="lineno">  513</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00514"></a><span class="lineno">  514</span>&#160;VX_API_ENTRY vx_node VX_API_CALL <a class="code" href="../../d6/d9a/group__group__cnn.html#ga6c4f56037d6a25692ef79887fdd952af">vxActivationLayer</a>(vx_graph graph, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> inputs, vx_enum func, vx_int32 a,vx_int32 b, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> outputs);</div>
<div class="line"><a name="l00515"></a><span class="lineno">  515</span>&#160;</div>
<div class="line"><a name="l00516"></a><span class="lineno">  516</span>&#160;</div>
<div class="line"><a name="l00517"></a><span class="lineno">  517</span>&#160;</div>
<div class="line"><a name="l00518"></a><span class="lineno">  518</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00519"></a><span class="lineno">  519</span>&#160;<span class="comment">/*! \brief [Graph] Performs element wise multiplications on element values in the input tensor data&#39;s with a scale.</span></div>
<div class="line"><a name="l00520"></a><span class="lineno">  520</span>&#160;<span class="comment">* \param [in] graph The handle to the graph.</span></div>
<div class="line"><a name="l00521"></a><span class="lineno">  521</span>&#160;<span class="comment">* \param [in] in1 input tensor data.</span></div>
<div class="line"><a name="l00522"></a><span class="lineno">  522</span>&#160;<span class="comment">* \param [in] in2 input tensor data, inputs must be of equal in dimensions.</span></div>
<div class="line"><a name="l00523"></a><span class="lineno">  523</span>&#160;<span class="comment">* else, If in one of the vx_mddata dimension is 1.</span></div>
<div class="line"><a name="l00524"></a><span class="lineno">  524</span>&#160;<span class="comment">* That dimension is considered as a const on all the dimension terms.</span></div>
<div class="line"><a name="l00525"></a><span class="lineno">  525</span>&#160;<span class="comment">* And will perform as if the values are duplicated on all terms in that dimensions.</span></div>
<div class="line"><a name="l00526"></a><span class="lineno">  526</span>&#160;<span class="comment">* After the expansion. The dimensions are equal. </span></div>
<div class="line"><a name="l00527"></a><span class="lineno">  527</span>&#160;<span class="comment">* \param [in] scale The scale value.</span></div>
<div class="line"><a name="l00528"></a><span class="lineno">  528</span>&#160;<span class="comment">* \param [in] overflow_policy A &lt;tt&gt;vx_convert_policy_e&lt;/tt&gt; enumeration.</span></div>
<div class="line"><a name="l00529"></a><span class="lineno">  529</span>&#160;<span class="comment">* \param [in] rounding_policy A &lt;tt&gt;vx_round_policy_e&lt;/tt&gt; enumeration.</span></div>
<div class="line"><a name="l00530"></a><span class="lineno">  530</span>&#160;<span class="comment">* \param [out] out The output tensor data with the same dimensions as the input tensor data&#39;s.</span></div>
<div class="line"><a name="l00531"></a><span class="lineno">  531</span>&#160;<span class="comment">* \ingroup group_tensor</span></div>
<div class="line"><a name="l00532"></a><span class="lineno">  532</span>&#160;<span class="comment">* \return &lt;tt&gt; vx_node&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00533"></a><span class="lineno">  533</span>&#160;<span class="comment">* \retval 0 Node could not be created.</span></div>
<div class="line"><a name="l00534"></a><span class="lineno">  534</span>&#160;<span class="comment">* \retval * Node handle.</span></div>
<div class="line"><a name="l00535"></a><span class="lineno">  535</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00536"></a><span class="lineno">  536</span>&#160;VX_API_ENTRY vx_node VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#ga5cb213b679b5a0ed643d6f9db36d2fc1">vxTensorMultiplyNode</a>(vx_graph graph, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> in1, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> in2, vx_scalar scale, vx_enum overflow_policy, vx_enum rounding_policy, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> out);</div>
<div class="line"><a name="l00537"></a><span class="lineno">  537</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00538"></a><span class="lineno">  538</span>&#160;<span class="comment">/*! \brief [Graph] Performs arithmetic addition on element values in the input tensor data&#39;s.</span></div>
<div class="line"><a name="l00539"></a><span class="lineno">  539</span>&#160;<span class="comment"> * \param [in] graph The handle to the graph.</span></div>
<div class="line"><a name="l00540"></a><span class="lineno">  540</span>&#160;<span class="comment"> * \param [in] in1 input tensor data,. </span></div>
<div class="line"><a name="l00541"></a><span class="lineno">  541</span>&#160;<span class="comment"> * \param [in] in2 input tensor data, inputs must be of equal in dimensions.</span></div>
<div class="line"><a name="l00542"></a><span class="lineno">  542</span>&#160;<span class="comment"> * else, If in one of the vx_mddata dimension is 1.</span></div>
<div class="line"><a name="l00543"></a><span class="lineno">  543</span>&#160;<span class="comment"> * That dimension is considered as a const on all the dimension terms.</span></div>
<div class="line"><a name="l00544"></a><span class="lineno">  544</span>&#160;<span class="comment"> * And will perform as if the values are duplicated on all terms in that dimensions.</span></div>
<div class="line"><a name="l00545"></a><span class="lineno">  545</span>&#160;<span class="comment"> * After the expansion. The dimensions are equal.</span></div>
<div class="line"><a name="l00546"></a><span class="lineno">  546</span>&#160;<span class="comment"> * \param [in] policy A vx_convert_policy_e enumeration.</span></div>
<div class="line"><a name="l00547"></a><span class="lineno">  547</span>&#160;<span class="comment"> * \param [out] out The output tensor data with the same dimensions as the input tensor data&#39;s.</span></div>
<div class="line"><a name="l00548"></a><span class="lineno">  548</span>&#160;<span class="comment"> * \ingroup group_tensor</span></div>
<div class="line"><a name="l00549"></a><span class="lineno">  549</span>&#160;<span class="comment"> * \return &lt;tt&gt; vx_node&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00550"></a><span class="lineno">  550</span>&#160;<span class="comment"> * \retval 0 Node could not be created.</span></div>
<div class="line"><a name="l00551"></a><span class="lineno">  551</span>&#160;<span class="comment"> * \retval * Node handle.</span></div>
<div class="line"><a name="l00552"></a><span class="lineno">  552</span>&#160;<span class="comment"> */</span></div>
<div class="line"><a name="l00553"></a><span class="lineno">  553</span>&#160;VX_API_ENTRY vx_node VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#ga9342ae0985396b3b2c6793eeb3c90336">vxTensorAddNode</a>(vx_graph graph, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> in1, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> in2, vx_enum policy, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> out);</div>
<div class="line"><a name="l00554"></a><span class="lineno">  554</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00555"></a><span class="lineno">  555</span>&#160;<span class="comment">/*! \brief [Graph] Performs arithmetic subtraction on element values in the input tensor data&#39;s.</span></div>
<div class="line"><a name="l00556"></a><span class="lineno">  556</span>&#160;<span class="comment">* \param [in] graph The handle to the graph.</span></div>
<div class="line"><a name="l00557"></a><span class="lineno">  557</span>&#160;<span class="comment">* \param [in] in1 input tensor data.</span></div>
<div class="line"><a name="l00558"></a><span class="lineno">  558</span>&#160;<span class="comment">* \param [in] in2 input tensor data, inputs must be of equal in dimensions.</span></div>
<div class="line"><a name="l00559"></a><span class="lineno">  559</span>&#160;<span class="comment">* else, If in one of the vx_mddata dimension is 1.</span></div>
<div class="line"><a name="l00560"></a><span class="lineno">  560</span>&#160;<span class="comment">* That dimension is considered as a const on all the dimension terms.</span></div>
<div class="line"><a name="l00561"></a><span class="lineno">  561</span>&#160;<span class="comment">* And will perform as if the values are duplicated on all terms in that dimensions.</span></div>
<div class="line"><a name="l00562"></a><span class="lineno">  562</span>&#160;<span class="comment">* After the expansion. The dimensions are equal.</span></div>
<div class="line"><a name="l00563"></a><span class="lineno">  563</span>&#160;<span class="comment">* \param [in] policy A vx_convert_policy_e enumeration.</span></div>
<div class="line"><a name="l00564"></a><span class="lineno">  564</span>&#160;<span class="comment">* \param [out] out The output tensor data with the same dimensions as the input tensor data&#39;s.</span></div>
<div class="line"><a name="l00565"></a><span class="lineno">  565</span>&#160;<span class="comment">* \ingroup group_tensor</span></div>
<div class="line"><a name="l00566"></a><span class="lineno">  566</span>&#160;<span class="comment">* \return &lt;tt&gt; vx_node&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00567"></a><span class="lineno">  567</span>&#160;<span class="comment">* \retval 0 Node could not be created.</span></div>
<div class="line"><a name="l00568"></a><span class="lineno">  568</span>&#160;<span class="comment">* \retval * Node handle.</span></div>
<div class="line"><a name="l00569"></a><span class="lineno">  569</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00570"></a><span class="lineno">  570</span>&#160;VX_API_ENTRY vx_node VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#gada7abc9351bb8db86d169f8a29b487de">vxTensorSubtractNode</a>(vx_graph graph, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> in1, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> in2, vx_enum policy, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> out);</div>
<div class="line"><a name="l00571"></a><span class="lineno">  571</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00572"></a><span class="lineno">  572</span>&#160;<span class="comment">/*! \brief [Graph] Performs LUT on element values in the input tensor data&#39;s.</span></div>
<div class="line"><a name="l00573"></a><span class="lineno">  573</span>&#160;<span class="comment">* \param [in] graph The handle to the graph.</span></div>
<div class="line"><a name="l00574"></a><span class="lineno">  574</span>&#160;<span class="comment">* \param [in] in1 input tensor data.</span></div>
<div class="line"><a name="l00575"></a><span class="lineno">  575</span>&#160;<span class="comment">* \param [in] lut of type &lt;tt&gt;vx_lut&lt;/tt&gt;</span></div>
<div class="line"><a name="l00576"></a><span class="lineno">  576</span>&#160;<span class="comment">* \param [out] out The output tensor data with the same dimensions as the input tensor data&#39;s.</span></div>
<div class="line"><a name="l00577"></a><span class="lineno">  577</span>&#160;<span class="comment">* \ingroup group_tensor</span></div>
<div class="line"><a name="l00578"></a><span class="lineno">  578</span>&#160;<span class="comment">* \return &lt;tt&gt; vx_node&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00579"></a><span class="lineno">  579</span>&#160;<span class="comment">* \retval 0 Node could not be created.</span></div>
<div class="line"><a name="l00580"></a><span class="lineno">  580</span>&#160;<span class="comment">* \retval * Node handle.</span></div>
<div class="line"><a name="l00581"></a><span class="lineno">  581</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00582"></a><span class="lineno">  582</span>&#160;VX_API_ENTRY vx_node VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#ga827364134e8934ae2c5859fefb5e9f97">vxTensorTableLookupNode</a>(vx_graph graph, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> in1, vx_lut lut, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> out);</div>
<div class="line"><a name="l00583"></a><span class="lineno">  583</span>&#160;</div>
<div class="line"><a name="l00584"></a><span class="lineno">  584</span>&#160;<span class="comment"></span></div>
<div class="line"><a name="l00585"></a><span class="lineno">  585</span>&#160;<span class="comment">/*! \brief [Graph] Performs transpose on the input tensor.</span></div>
<div class="line"><a name="l00586"></a><span class="lineno">  586</span>&#160;<span class="comment">* The node transpose the tensor according to a specified 2 indexes in the tensor (0-based indexing)</span></div>
<div class="line"><a name="l00587"></a><span class="lineno">  587</span>&#160;<span class="comment">* \param [in] graph The handle to the graph.</span></div>
<div class="line"><a name="l00588"></a><span class="lineno">  588</span>&#160;<span class="comment">* \param [in] in input tensor data,</span></div>
<div class="line"><a name="l00589"></a><span class="lineno">  589</span>&#160;<span class="comment">* \param [out] out output tensor data,</span></div>
<div class="line"><a name="l00590"></a><span class="lineno">  590</span>&#160;<span class="comment">* \param [in] dim1 that is transposed with dim 2.</span></div>
<div class="line"><a name="l00591"></a><span class="lineno">  591</span>&#160;<span class="comment">* \param [in] dim2 that is transposed with dim 1.</span></div>
<div class="line"><a name="l00592"></a><span class="lineno">  592</span>&#160;<span class="comment">* \ingroup group_tensor</span></div>
<div class="line"><a name="l00593"></a><span class="lineno">  593</span>&#160;<span class="comment">* \return &lt;tt&gt; vx_node&lt;/tt&gt;.</span></div>
<div class="line"><a name="l00594"></a><span class="lineno">  594</span>&#160;<span class="comment">* \retval 0 Node could not be created.</span></div>
<div class="line"><a name="l00595"></a><span class="lineno">  595</span>&#160;<span class="comment">* \retval * Node handle.</span></div>
<div class="line"><a name="l00596"></a><span class="lineno">  596</span>&#160;<span class="comment">*/</span></div>
<div class="line"><a name="l00597"></a><span class="lineno">  597</span>&#160;VX_API_ENTRY vx_node VX_API_CALL <a class="code" href="../../dd/dad/group__group__tensor.html#gab8095ad90a57f4d45fc27cb57837577a">vxTensorTransposeNode</a>(vx_graph graph, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> in, <a class="code" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> out, vx_uint32 dim1, vx_uint32 dim2);</div>
<div class="line"><a name="l00598"></a><span class="lineno">  598</span>&#160;</div>
<div class="line"><a name="l00599"></a><span class="lineno">  599</span>&#160;</div>
<div class="line"><a name="l00600"></a><span class="lineno">  600</span>&#160;</div>
<div class="line"><a name="l00601"></a><span class="lineno">  601</span>&#160;</div>
<div class="line"><a name="l00602"></a><span class="lineno">  602</span>&#160;<span class="preprocessor">#endif</span></div>
<div class="ttc" id="group__group__tensor_html_gga284570428744ab05195daa2707dcb688a0930a6e574a2c12049d60c0daea43778"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#gga284570428744ab05195daa2707dcb688a0930a6e574a2c12049d60c0daea43778">VX_TENSOR_NUM_OF_DIMS</a></div><div class="ttdoc">Number of dimensions. </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00058">vx_khr_cnn.h:58</a></div></div>
<div class="ttc" id="group__group__cnn_html_ga0b2cc8c5e172128a3014b56a1ae7b173"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#ga0b2cc8c5e172128a3014b56a1ae7b173">vxPoolingLayer</a></div><div class="ttdeci">vx_node vxPoolingLayer(vx_graph graph, vx_tensor inputs, vx_enum pool_type, vx_uint32 pool_size_x, vx_uint32 pool_size_y, vx_uint32 pool_pad_x, vx_uint32 pool_pad_y, vx_enum rounding, vx_tensor outputs)</div><div class="ttdoc">[Graph] Creates a Convolutional Network Pooling Layer Node. </div></div>
<div class="ttc" id="group__group__cnn_html_gaf940a27e357e2836af6a6492148f4c93"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#gaf940a27e357e2836af6a6492148f4c93">vxFullyConnectedLayer</a></div><div class="ttdeci">vx_node vxFullyConnectedLayer(vx_graph graph, vx_tensor inputs, vx_tensor weights, vx_tensor biases, vx_uint32 pad, vx_uint8 accumulator_bits, vx_enum overflow_policy, vx_enum rounding_policy, vx_enum down_scale_size_rounding, vx_tensor outputs)</div><div class="ttdoc">[Graph] Creates a Fully connected Convolutional Network Layer Node. </div></div>
<div class="ttc" id="group__group__cnn_html_ga76a3a94a73992017519d7dc01e81c476"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#ga76a3a94a73992017519d7dc01e81c476">vxSoftmaxLayer</a></div><div class="ttdeci">vx_node vxSoftmaxLayer(vx_graph graph, vx_tensor inputs, vx_tensor outputs)</div><div class="ttdoc">[Graph] Creates a Convolutional Network Softmax Layer Node. </div></div>
<div class="ttc" id="group__group__cnn_html_ggab041071cfd76f089eba37379db229fb7a530049de61a14d590562f8840a19be2e"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#ggab041071cfd76f089eba37379db229fb7a530049de61a14d590562f8840a19be2e">VX_CONVOLUTIONAL_NETWORK_POOLING_MAX</a></div><div class="ttdoc">max pooling </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00126">vx_khr_cnn.h:126</a></div></div>
<div class="ttc" id="group__group__tensor_html_gga284570428744ab05195daa2707dcb688a589cd47c54ab8a962b46ae0f8a9e67be"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#gga284570428744ab05195daa2707dcb688a589cd47c54ab8a962b46ae0f8a9e67be">VX_TENSOR_DIMS</a></div><div class="ttdoc">Dimension sizes. </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00060">vx_khr_cnn.h:60</a></div></div>
<div class="ttc" id="group__group__tensor_html_ga2aaa8f0282d709a2bc3067737390ff5a"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#ga2aaa8f0282d709a2bc3067737390ff5a">vxCreateTensorView</a></div><div class="ttdeci">vx_tensor_view vxCreateTensorView(vx_context context, vx_uint32 *view_array_start, vx_uint32 *view_array_end, vx_uint8 numViewDimensions)</div><div class="ttdoc">Create an opaque reference to a tensor view object. </div></div>
<div class="ttc" id="group__group__tensor_html_gad7487061cf90c8b1b1e2538bf79d3eaf"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#gad7487061cf90c8b1b1e2538bf79d3eaf">vxCreateTensorAddressing</a></div><div class="ttdeci">vx_tensor_addressing vxCreateTensorAddressing(vx_context context, vx_uint32 *addressing_array_dimension, vx_uint32 *addressing_array_stride, vx_uint8 numViewDimensions)</div><div class="ttdoc">Create an opaque reference to a tensor addressing object. </div></div>
<div class="ttc" id="group__group__cnn_html_ggaea14552ad4e594583c11d8e6163557c1a71594a34842bc09f8f876809d410cb52"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#ggaea14552ad4e594583c11d8e6163557c1a71594a34842bc09f8f876809d410cb52">VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR</a></div><div class="ttdoc">floor rounding </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00113">vx_khr_cnn.h:113</a></div></div>
<div class="ttc" id="group__group__cnn_html_ggad8a98bdea5d6620d3829b272bcccf9abaaae1a24ba2c0456e0114c8526d3e550d"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#ggad8a98bdea5d6620d3829b272bcccf9abaaae1a24ba2c0456e0114c8526d3e550d">VX_CONVOLUTIONAL_NETWORK_NORM_SAME_MAP</a></div><div class="ttdoc">normalization is done on same IFM </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00138">vx_khr_cnn.h:138</a></div></div>
<div class="ttc" id="group__group__tensor_html_ga413cc779500ae059e474b1b58428c3f9"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#ga413cc779500ae059e474b1b58428c3f9">vxCreateImageObjectArrayFromTensor</a></div><div class="ttdeci">vx_object_array vxCreateImageObjectArrayFromTensor(vx_tensor tensor, vx_rectangle_t rect, vx_uint32 array_size, vx_uint32 stride, vx_df_image image_format)</div><div class="ttdoc">Creates an array of images into the multi-dimension data, this can be a adjacent 2D images or not dep...</div></div>
<div class="ttc" id="group__group__tensor_html_ga4ec0834c08f2a94465891d3ad440729d"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#ga4ec0834c08f2a94465891d3ad440729d">vxReleaseTensorView</a></div><div class="ttdeci">vx_status vxReleaseTensorView(vx_tensor_view *tensor_view)</div><div class="ttdoc">Releases a reference to a tensor data view object. The object may not be garbage collected until its ...</div></div>
<div class="ttc" id="group__group__cnn_html_gab041071cfd76f089eba37379db229fb7"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#gab041071cfd76f089eba37379db229fb7">vx_convolutional_network_pooling_type_e</a></div><div class="ttdeci">vx_convolutional_network_pooling_type_e</div><div class="ttdoc">The Convolutional Network pooling type list. </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00123">vx_khr_cnn.h:123</a></div></div>
<div class="ttc" id="group__group__cnn_html_ga16743849220a5ac0539ed4272b33a7dc"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#ga16743849220a5ac0539ed4272b33a7dc">vx_convolutional_network_activation_func_e</a></div><div class="ttdeci">vx_convolutional_network_activation_func_e</div><div class="ttdoc">The Convolutional Network activation functions list. </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00161">vx_khr_cnn.h:161</a></div></div>
<div class="ttc" id="group__group__cnn_html_gad8a98bdea5d6620d3829b272bcccf9ab"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#gad8a98bdea5d6620d3829b272bcccf9ab">vx_convolutional_network_norm_type_e</a></div><div class="ttdeci">vx_convolutional_network_norm_type_e</div><div class="ttdoc">The Convolutional Network normalization type list. </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00135">vx_khr_cnn.h:135</a></div></div>
<div class="ttc" id="group__group__tensor_html_ga284570428744ab05195daa2707dcb688"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#ga284570428744ab05195daa2707dcb688">vx_tensor_attribute_e</a></div><div class="ttdeci">vx_tensor_attribute_e</div><div class="ttdoc">tensor Data attributes. </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00055">vx_khr_cnn.h:55</a></div></div>
<div class="ttc" id="group__group__tensor_html_gaa96e480cdfdcb642d0f3f944ea6fec4f"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#gaa96e480cdfdcb642d0f3f944ea6fec4f">vxQueryTensor</a></div><div class="ttdeci">vx_status vxQueryTensor(vx_tensor tensor, vx_enum attribute, void *ptr, vx_size size)</div><div class="ttdoc">Retrieves various attributes of a tensor data. </div></div>
<div class="ttc" id="group__group__cnn_html_gaea14552ad4e594583c11d8e6163557c1"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#gaea14552ad4e594583c11d8e6163557c1">vx_convolutional_networks_rounding_type_e</a></div><div class="ttdeci">vx_convolutional_networks_rounding_type_e</div><div class="ttdoc">The Convolutional Network down scaling size rounding type list. </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00110">vx_khr_cnn.h:110</a></div></div>
<div class="ttc" id="group__group__tensor_html_gga284570428744ab05195daa2707dcb688aba74715b3a066ea13959fda51cb7cb91"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#gga284570428744ab05195daa2707dcb688aba74715b3a066ea13959fda51cb7cb91">VX_TENSOR_FIXED_POINT_POS</a></div><div class="ttdoc">fixed point position when the input element type is int16. </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00064">vx_khr_cnn.h:64</a></div></div>
<div class="ttc" id="group__group__tensor_html_ga9342ae0985396b3b2c6793eeb3c90336"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#ga9342ae0985396b3b2c6793eeb3c90336">vxTensorAddNode</a></div><div class="ttdeci">vx_node vxTensorAddNode(vx_graph graph, vx_tensor in1, vx_tensor in2, vx_enum policy, vx_tensor out)</div><div class="ttdoc">[Graph] Performs arithmetic addition on element values in the input tensor data&#39;s. </div></div>
<div class="ttc" id="group__group__tensor_html_gafbdcb72a17b7d740db68ae9464d5d6fd"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#gafbdcb72a17b7d740db68ae9464d5d6fd">vxReleaseTensor</a></div><div class="ttdeci">vx_status vxReleaseTensor(vx_tensor *tensor)</div><div class="ttdoc">Releases a reference to a tensor data object. The object may not be garbage collected until its total...</div></div>
<div class="ttc" id="group__group__tensor_html_ga3f63a5a6f4a3a08eb45a35211956c44e"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#ga3f63a5a6f4a3a08eb45a35211956c44e">vxCreateTensorFromView</a></div><div class="ttdeci">vx_tensor vxCreateTensorFromView(vx_tensor tensor, vx_tensor_view view)</div><div class="ttdoc">Creates a tensor data from another tensor data given a view. This second reference refers to the data...</div></div>
<div class="ttc" id="group__group__tensor_html_gaefbe70a40c695d79ebeb2bc550351e04"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#gaefbe70a40c695d79ebeb2bc550351e04">vxCreateTensor</a></div><div class="ttdeci">vx_tensor vxCreateTensor(vx_context context, vx_uint32 num_of_dims, vx_uint32 *sizes, vx_enum data_format, vx_uint8 fixed_point_pos)</div><div class="ttdoc">Creates an opaque reference to a tensor data buffer. </div></div>
<div class="ttc" id="group__group__tensor_html_ga6f6c0db1e3f276331707a50522294e51"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#ga6f6c0db1e3f276331707a50522294e51">vx_tensor_addressing</a></div><div class="ttdeci">struct _vx_tensor_addressing_t * vx_tensor_addressing</div><div class="ttdoc">The addressing of a tensor view patch structure is used by the Host only to address elements in a ten...</div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00103">vx_khr_cnn.h:103</a></div></div>
<div class="ttc" id="group__group__cnn_html_ggaea14552ad4e594583c11d8e6163557c1a0548ecdc1205f9d8031fc557e41bf2f2"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#ggaea14552ad4e594583c11d8e6163557c1a0548ecdc1205f9d8031fc557e41bf2f2">VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING</a></div><div class="ttdoc">ceil rounding </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00115">vx_khr_cnn.h:115</a></div></div>
<div class="ttc" id="group__group__cnn_html_ggab041071cfd76f089eba37379db229fb7ac486a0bcf89d55a1a0bfffff548aad76"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#ggab041071cfd76f089eba37379db229fb7ac486a0bcf89d55a1a0bfffff548aad76">VX_CONVOLUTIONAL_NETWORK_POOLING_AVG</a></div><div class="ttdoc">average pooling </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00128">vx_khr_cnn.h:128</a></div></div>
<div class="ttc" id="group__group__tensor_html_ga827364134e8934ae2c5859fefb5e9f97"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#ga827364134e8934ae2c5859fefb5e9f97">vxTensorTableLookupNode</a></div><div class="ttdeci">vx_node vxTensorTableLookupNode(vx_graph graph, vx_tensor in1, vx_lut lut, vx_tensor out)</div><div class="ttdoc">[Graph] Performs LUT on element values in the input tensor data&#39;s. </div></div>
<div class="ttc" id="group__group__cnn_html_ggad8a98bdea5d6620d3829b272bcccf9aba224c50ee37b0e4b1a8866c06939a4d95"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#ggad8a98bdea5d6620d3829b272bcccf9aba224c50ee37b0e4b1a8866c06939a4d95">VX_CONVOLUTIONAL_NETWORK_NORM_ACROSS_MAPS</a></div><div class="ttdoc">Normalization is done across different IFMs. </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00140">vx_khr_cnn.h:140</a></div></div>
<div class="ttc" id="group__group__tensor_html_ga372396aa9489f7fe30f9af0384d18340"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#ga372396aa9489f7fe30f9af0384d18340">vx_tensor_view</a></div><div class="ttdeci">struct _vx_tensor_view_t * vx_tensor_view</div><div class="ttdoc">The multi dimensional view data structure. </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00096">vx_khr_cnn.h:96</a></div></div>
<div class="ttc" id="group__group__tensor_html_gga7f849c7db71cc56b2a554d7d342ff421af8fd07fd2755ca696d0cc50cbf1f67fb"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#gga7f849c7db71cc56b2a554d7d342ff421af8fd07fd2755ca696d0cc50cbf1f67fb">VX_CONTEXT_MAX_TENSOR_DIMENSIONS</a></div><div class="ttdoc">tensor Data max num of dimensions supported by HW. </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00073">vx_khr_cnn.h:73</a></div></div>
<div class="ttc" id="group__group__tensor_html_gab8095ad90a57f4d45fc27cb57837577a"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#gab8095ad90a57f4d45fc27cb57837577a">vxTensorTransposeNode</a></div><div class="ttdeci">vx_node vxTensorTransposeNode(vx_graph graph, vx_tensor in, vx_tensor out, vx_uint32 dim1, vx_uint32 dim2)</div><div class="ttdoc">[Graph] Performs transpose on the input tensor. The node transpose the tensor according to a specifie...</div></div>
<div class="ttc" id="group__group__tensor_html_gabacda16928cb3ab9714402c64a9f9710"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#gabacda16928cb3ab9714402c64a9f9710">vxCopyTensorPatch</a></div><div class="ttdeci">vx_status vxCopyTensorPatch(vx_tensor tensor, vx_tensor_view view, vx_tensor_addressing user_addr, void *user_ptr, vx_enum usage, vx_enum user_mem_type)</div><div class="ttdoc">Allows the application to copy a view patch from/into an tensor object . </div></div>
<div class="ttc" id="group__group__tensor_html_ga5cb213b679b5a0ed643d6f9db36d2fc1"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#ga5cb213b679b5a0ed643d6f9db36d2fc1">vxTensorMultiplyNode</a></div><div class="ttdeci">vx_node vxTensorMultiplyNode(vx_graph graph, vx_tensor in1, vx_tensor in2, vx_scalar scale, vx_enum overflow_policy, vx_enum rounding_policy, vx_tensor out)</div><div class="ttdoc">[Graph] Performs element wise multiplications on element values in the input tensor data&#39;s with a sca...</div></div>
<div class="ttc" id="group__group__cnn_html_ga870c106e8ceb4c118692c6f754f75f43"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#ga870c106e8ceb4c118692c6f754f75f43">vxConvolutionLayer</a></div><div class="ttdeci">vx_node vxConvolutionLayer(vx_graph graph, vx_tensor inputs, vx_tensor weights, vx_tensor biases, vx_uint32 pad_x, vx_uint32 pad_y, vx_uint8 accumulator_bits, vx_enum overflow_policy, vx_enum rounding_policy, vx_enum down_scale_size_rounding, vx_tensor outputs)</div><div class="ttdoc">[Graph] Creates a Convolutional Network Convolution Layer Node. </div></div>
<div class="ttc" id="group__group__cnn_html_ga6c4f56037d6a25692ef79887fdd952af"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#ga6c4f56037d6a25692ef79887fdd952af">vxActivationLayer</a></div><div class="ttdeci">vx_node vxActivationLayer(vx_graph graph, vx_tensor inputs, vx_enum func, vx_int32 a, vx_int32 b, vx_tensor outputs)</div><div class="ttdoc">[Graph] Creates a Convolutional Network Activation Layer Node. </div></div>
<div class="ttc" id="group__group__tensor_html_gada7abc9351bb8db86d169f8a29b487de"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#gada7abc9351bb8db86d169f8a29b487de">vxTensorSubtractNode</a></div><div class="ttdeci">vx_node vxTensorSubtractNode(vx_graph graph, vx_tensor in1, vx_tensor in2, vx_enum policy, vx_tensor out)</div><div class="ttdoc">[Graph] Performs arithmetic subtraction on element values in the input tensor data&#39;s. </div></div>
<div class="ttc" id="group__group__tensor_html_ga61ce36a3376598180334b1a424dcdcf9"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a></div><div class="ttdeci">struct _vx_tensor_t * vx_tensor</div><div class="ttdoc">The multidimensional data object (Tensor). </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00089">vx_khr_cnn.h:89</a></div></div>
<div class="ttc" id="group__group__cnn_html_ga2269eca24047c7b564ab7708a8420afa"><div class="ttname"><a href="../../d6/d9a/group__group__cnn.html#ga2269eca24047c7b564ab7708a8420afa">vxNormalizationLayer</a></div><div class="ttdeci">vx_node vxNormalizationLayer(vx_graph graph, vx_tensor inputs, vx_enum type, vx_uint32 norm_size, vx_float32 alpha, vx_float32 beta, vx_tensor outputs)</div><div class="ttdoc">[Graph] Creates a Convolutional Network Normalization Layer Node. </div></div>
<div class="ttc" id="group__group__tensor_html_ga30b8f5de5443a53f65746ea5dd0050c9"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#ga30b8f5de5443a53f65746ea5dd0050c9">vxReleaseTensorAddressing</a></div><div class="ttdeci">vx_status vxReleaseTensorAddressing(vx_tensor_addressing *tensor_addr)</div><div class="ttdoc">Releases a reference to a tensor data addressing object. The object may not be garbage collected unti...</div></div>
<div class="ttc" id="group__group__tensor_html_ga7f849c7db71cc56b2a554d7d342ff421"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#ga7f849c7db71cc56b2a554d7d342ff421">vx_context_attribute_e</a></div><div class="ttdeci">vx_context_attribute_e</div><div class="ttdoc">A list of context attributes. </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00071">vx_khr_cnn.h:71</a></div></div>
<div class="ttc" id="group__group__tensor_html_gga284570428744ab05195daa2707dcb688a69071022451108c3e0dd6e6c978f30f7"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#gga284570428744ab05195daa2707dcb688a69071022451108c3e0dd6e6c978f30f7">VX_TENSOR_DATA_TYPE</a></div><div class="ttdoc">tensor Data element data type. vx_type_e </div><div class="ttdef"><b>Definition:</b> <a href="../../df/d74/vx__khr__cnn_8h_source.html#l00062">vx_khr_cnn.h:62</a></div></div>
<div class="ttc" id="group__group__tensor_html_gad7451299a63894cd952324c9c073609c"><div class="ttname"><a href="../../dd/dad/group__group__tensor.html#gad7451299a63894cd952324c9c073609c">vxCreateVirtualTensor</a></div><div class="ttdeci">vx_tensor vxCreateVirtualTensor(vx_graph graph, vx_uint32 num_of_dims, vx_uint32 *sizes, vx_enum data_format, vx_uint8 fixed_point_pos)</div><div class="ttdoc">Creates an opaque reference to a tensor data buffer with no direct user access. This function allows ...</div></div>
</div><!-- fragment --></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="navelem"><a class="el" href="../../dir_d44c64559bbebec7f509842c48db8b23.html">include</a></li><li class="navelem"><a class="el" href="../../dir_a3324bfc7e66844c57dfd3bddf0b96d4.html">VX</a></li><li class="navelem"><b>vx_khr_cnn.h</b></li>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="../../doxygen.png" alt="doxygen"/></a> 1.8.6 </li>
  </ul>
</div>
</body>
</html>
