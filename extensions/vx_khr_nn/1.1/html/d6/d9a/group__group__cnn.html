<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.6"/>
<title>OpenVX Neural Network Extension: Extension: Deep Convolutional Networks API</title>
<link href="../../tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../jquery.js"></script>
<script type="text/javascript" src="../../dynsections.js"></script>
<link href="../../navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../resize.js"></script>
<script type="text/javascript" src="../../navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="../../search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="../../search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX","output/HTML-CSS"],
});
</script><script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js"></script>
<link href="../../doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="../../OpenVX_Color_119x55.png"/></td>
  <td style="padding-left: 0.5em;">
   <div id="projectname">OpenVX Neural Network Extension
   &#160;<span id="projectnumber">7505566</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.6 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "../../search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="../../index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="../../modules.html"><span>Modules</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="../../search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="../../search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('d6/d9a/group__group__cnn.html','../../');});
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Groups</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="summary">
<a href="#enum-members">Enumerations</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle">
<div class="title">Extension: Deep Convolutional Networks API</div>  </div>
</div><!--header-->
<div class="contents">

<p>Convolutional Network Nodes.  
<a href="#details">More...</a></p>
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="enum-members"></a>
Enumerations</h2></td></tr>
<tr class="memitem:ga16743849220a5ac0539ed4272b33a7dc"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d6/d9a/group__group__cnn.html#ga16743849220a5ac0539ed4272b33a7dc">vx_convolutional_network_activation_func_e</a> { <br/>
&#160;&#160;<b>VX_CONVOLUTIONAL_NETWORK_ACTIVATION_LOGISTIC</b> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC &lt;&lt; 12)) + 0x0, 
<br/>
&#160;&#160;<b>VX_CONVOLUTIONAL_NETWORK_ACTIVATION_HYPERBOLIC_TAN</b> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC &lt;&lt; 12)) + 0x1, 
<br/>
&#160;&#160;<b>VX_CONVOLUTIONAL_NETWORK_ACTIVATION_RELU</b> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC &lt;&lt; 12)) + 0x2, 
<br/>
&#160;&#160;<b>VX_CONVOLUTIONAL_NETWORK_ACTIVATION_BRELU</b> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC &lt;&lt; 12)) + 0x3, 
<br/>
&#160;&#160;<b>VX_CONVOLUTIONAL_NETWORK_ACTIVATION_SOFTRELU</b> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC &lt;&lt; 12)) + 0x4, 
<br/>
&#160;&#160;<b>VX_CONVOLUTIONAL_NETWORK_ACTIVATION_ABS</b> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC &lt;&lt; 12)) + 0x5, 
<br/>
&#160;&#160;<b>VX_CONVOLUTIONAL_NETWORK_ACTIVATION_SQUARE</b> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC &lt;&lt; 12)) + 0x6, 
<br/>
&#160;&#160;<b>VX_CONVOLUTIONAL_NETWORK_ACTIVATION_SQRT</b> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC &lt;&lt; 12)) + 0x7, 
<br/>
&#160;&#160;<b>VX_CONVOLUTIONAL_NETWORK_ACTIVATION_LINEAR</b> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_ACTIVATION_FUNC &lt;&lt; 12)) + 0x8
<br/>
 }</td></tr>
<tr class="memdesc:ga16743849220a5ac0539ed4272b33a7dc"><td class="mdescLeft">&#160;</td><td class="mdescRight">The Convolutional Network activation functions list.  <a href="../../d6/d9a/group__group__cnn.html#ga16743849220a5ac0539ed4272b33a7dc">More...</a><br/></td></tr>
<tr class="separator:ga16743849220a5ac0539ed4272b33a7dc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gad8a98bdea5d6620d3829b272bcccf9ab"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d6/d9a/group__group__cnn.html#gad8a98bdea5d6620d3829b272bcccf9ab">vx_convolutional_network_norm_type_e</a> { <br/>
&#160;&#160;<a class="el" href="../../d6/d9a/group__group__cnn.html#ggad8a98bdea5d6620d3829b272bcccf9abaaae1a24ba2c0456e0114c8526d3e550d">VX_CONVOLUTIONAL_NETWORK_NORM_SAME_MAP</a> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_NORM_TYPE &lt;&lt; 12)) + 0x0, 
<br/>
&#160;&#160;<a class="el" href="../../d6/d9a/group__group__cnn.html#ggad8a98bdea5d6620d3829b272bcccf9aba224c50ee37b0e4b1a8866c06939a4d95">VX_CONVOLUTIONAL_NETWORK_NORM_ACROSS_MAPS</a> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_NORM_TYPE &lt;&lt; 12)) + 0x1
<br/>
 }</td></tr>
<tr class="memdesc:gad8a98bdea5d6620d3829b272bcccf9ab"><td class="mdescLeft">&#160;</td><td class="mdescRight">The Convolutional Network normalization type list.  <a href="../../d6/d9a/group__group__cnn.html#gad8a98bdea5d6620d3829b272bcccf9ab">More...</a><br/></td></tr>
<tr class="separator:gad8a98bdea5d6620d3829b272bcccf9ab"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gab041071cfd76f089eba37379db229fb7"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d6/d9a/group__group__cnn.html#gab041071cfd76f089eba37379db229fb7">vx_convolutional_network_pooling_type_e</a> { <br/>
&#160;&#160;<a class="el" href="../../d6/d9a/group__group__cnn.html#ggab041071cfd76f089eba37379db229fb7a530049de61a14d590562f8840a19be2e">VX_CONVOLUTIONAL_NETWORK_POOLING_MAX</a> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_POOL_TYPE &lt;&lt; 12)) + 0x0, 
<br/>
&#160;&#160;<a class="el" href="../../d6/d9a/group__group__cnn.html#ggab041071cfd76f089eba37379db229fb7ac486a0bcf89d55a1a0bfffff548aad76">VX_CONVOLUTIONAL_NETWORK_POOLING_AVG</a> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_POOL_TYPE &lt;&lt; 12)) + 0x1
<br/>
 }</td></tr>
<tr class="memdesc:gab041071cfd76f089eba37379db229fb7"><td class="mdescLeft">&#160;</td><td class="mdescRight">The Convolutional Network pooling type list.  <a href="../../d6/d9a/group__group__cnn.html#gab041071cfd76f089eba37379db229fb7">More...</a><br/></td></tr>
<tr class="separator:gab041071cfd76f089eba37379db229fb7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaea14552ad4e594583c11d8e6163557c1"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d6/d9a/group__group__cnn.html#gaea14552ad4e594583c11d8e6163557c1">vx_convolutional_networks_rounding_type_e</a> { <br/>
&#160;&#160;<a class="el" href="../../d6/d9a/group__group__cnn.html#ggaea14552ad4e594583c11d8e6163557c1a71594a34842bc09f8f876809d410cb52">VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR</a> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_ROUNDING_TYPE &lt;&lt; 12)) + 0x0, 
<br/>
&#160;&#160;<a class="el" href="../../d6/d9a/group__group__cnn.html#ggaea14552ad4e594583c11d8e6163557c1a0548ecdc1205f9d8031fc557e41bf2f2">VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING</a> = ((( VX_ID_KHRONOS ) &lt;&lt; 20) | ( VX_ENUM_CONVOLUTIONAL_NETWORK_ROUNDING_TYPE &lt;&lt; 12)) + 0x1
<br/>
 }</td></tr>
<tr class="memdesc:gaea14552ad4e594583c11d8e6163557c1"><td class="mdescLeft">&#160;</td><td class="mdescRight">The Convolutional Network down scaling size rounding type list.  <a href="../../d6/d9a/group__group__cnn.html#gaea14552ad4e594583c11d8e6163557c1">More...</a><br/></td></tr>
<tr class="separator:gaea14552ad4e594583c11d8e6163557c1"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ga6c4f56037d6a25692ef79887fdd952af"><td class="memItemLeft" align="right" valign="top">vx_node&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d6/d9a/group__group__cnn.html#ga6c4f56037d6a25692ef79887fdd952af">vxActivationLayer</a> (vx_graph graph, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> inputs, vx_enum func, vx_int32 a, vx_int32 b, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> outputs)</td></tr>
<tr class="memdesc:ga6c4f56037d6a25692ef79887fdd952af"><td class="mdescLeft">&#160;</td><td class="mdescRight">[Graph] Creates a Convolutional Network Activation Layer Node.  <a href="#ga6c4f56037d6a25692ef79887fdd952af">More...</a><br/></td></tr>
<tr class="separator:ga6c4f56037d6a25692ef79887fdd952af"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga870c106e8ceb4c118692c6f754f75f43"><td class="memItemLeft" align="right" valign="top">vx_node&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d6/d9a/group__group__cnn.html#ga870c106e8ceb4c118692c6f754f75f43">vxConvolutionLayer</a> (vx_graph graph, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> inputs, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> weights, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> biases, vx_uint32 pad_x, vx_uint32 pad_y, vx_uint8 accumulator_bits, vx_enum overflow_policy, vx_enum rounding_policy, vx_enum down_scale_size_rounding, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> outputs)</td></tr>
<tr class="memdesc:ga870c106e8ceb4c118692c6f754f75f43"><td class="mdescLeft">&#160;</td><td class="mdescRight">[Graph] Creates a Convolutional Network Convolution Layer Node.  <a href="#ga870c106e8ceb4c118692c6f754f75f43">More...</a><br/></td></tr>
<tr class="separator:ga870c106e8ceb4c118692c6f754f75f43"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaf940a27e357e2836af6a6492148f4c93"><td class="memItemLeft" align="right" valign="top">vx_node&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d6/d9a/group__group__cnn.html#gaf940a27e357e2836af6a6492148f4c93">vxFullyConnectedLayer</a> (vx_graph graph, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> inputs, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> weights, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> biases, vx_uint32 pad, vx_uint8 accumulator_bits, vx_enum overflow_policy, vx_enum rounding_policy, vx_enum down_scale_size_rounding, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> outputs)</td></tr>
<tr class="memdesc:gaf940a27e357e2836af6a6492148f4c93"><td class="mdescLeft">&#160;</td><td class="mdescRight">[Graph] Creates a Fully connected Convolutional Network Layer Node.  <a href="#gaf940a27e357e2836af6a6492148f4c93">More...</a><br/></td></tr>
<tr class="separator:gaf940a27e357e2836af6a6492148f4c93"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga2269eca24047c7b564ab7708a8420afa"><td class="memItemLeft" align="right" valign="top">vx_node&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d6/d9a/group__group__cnn.html#ga2269eca24047c7b564ab7708a8420afa">vxNormalizationLayer</a> (vx_graph graph, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> inputs, vx_enum type, vx_uint32 norm_size, vx_float32 alpha, vx_float32 beta, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> outputs)</td></tr>
<tr class="memdesc:ga2269eca24047c7b564ab7708a8420afa"><td class="mdescLeft">&#160;</td><td class="mdescRight">[Graph] Creates a Convolutional Network Normalization Layer Node.  <a href="#ga2269eca24047c7b564ab7708a8420afa">More...</a><br/></td></tr>
<tr class="separator:ga2269eca24047c7b564ab7708a8420afa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga0b2cc8c5e172128a3014b56a1ae7b173"><td class="memItemLeft" align="right" valign="top">vx_node&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d6/d9a/group__group__cnn.html#ga0b2cc8c5e172128a3014b56a1ae7b173">vxPoolingLayer</a> (vx_graph graph, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> inputs, vx_enum pool_type, vx_uint32 pool_size_x, vx_uint32 pool_size_y, vx_uint32 pool_pad_x, vx_uint32 pool_pad_y, vx_enum rounding, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> outputs)</td></tr>
<tr class="memdesc:ga0b2cc8c5e172128a3014b56a1ae7b173"><td class="mdescLeft">&#160;</td><td class="mdescRight">[Graph] Creates a Convolutional Network Pooling Layer Node.  <a href="#ga0b2cc8c5e172128a3014b56a1ae7b173">More...</a><br/></td></tr>
<tr class="separator:ga0b2cc8c5e172128a3014b56a1ae7b173"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga76a3a94a73992017519d7dc01e81c476"><td class="memItemLeft" align="right" valign="top">vx_node&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="../../d6/d9a/group__group__cnn.html#ga76a3a94a73992017519d7dc01e81c476">vxSoftmaxLayer</a> (vx_graph graph, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> inputs, <a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a> outputs)</td></tr>
<tr class="memdesc:ga76a3a94a73992017519d7dc01e81c476"><td class="mdescLeft">&#160;</td><td class="mdescRight">[Graph] Creates a Convolutional Network Softmax Layer Node.  <a href="#ga76a3a94a73992017519d7dc01e81c476">More...</a><br/></td></tr>
<tr class="separator:ga76a3a94a73992017519d7dc01e81c476"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<p>Convolutional Network Nodes. </p>
<h2 class="groupheader">Enumeration Type Documentation</h2>
<a class="anchor" id="ga16743849220a5ac0539ed4272b33a7dc"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="../../d6/d9a/group__group__cnn.html#ga16743849220a5ac0539ed4272b33a7dc">vx_convolutional_network_activation_func_e</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The Convolutional Network activation functions list. </p>
<table class="doxtable">
<tr>
<td><b>Function name </b> </td><td><b>Mathematical definition</b> </td><td><b>Parameters</b> </td><td><b>Parameters type</b> </td></tr>
<tr>
<td>logistic </td><td>\(f(x)=1/(1+e^{-x}) \) </td><td></td><td></td></tr>
<tr>
<td>hyperbolic tangent </td><td>\(f(x)=a\cdot tanh(b\cdot x) \) </td><td>a,b </td><td>VX_INT32 </td></tr>
<tr>
<td>relu </td><td>\(f(x)=max(0,x)\) </td><td></td><td></td></tr>
<tr>
<td>bounded relu </td><td>\(f(x)=min(a,max(0,x)) \) </td><td>a </td><td>VX_INT32 </td></tr>
<tr>
<td>soft relu </td><td>\(f(x)=log(1+e^{x}) \) </td><td></td><td></td></tr>
<tr>
<td>abs </td><td>\(f(x)=\mid x\mid \) </td><td></td><td></td></tr>
<tr>
<td>square </td><td>\(f(x)= x^2 \) </td><td></td><td></td></tr>
<tr>
<td>square root </td><td>\(f(x)=\sqrt{x} \) </td><td></td><td></td></tr>
<tr>
<td>linear </td><td>\(f(x)=ax+b \) </td><td>a,b </td><td>VX_INT32 </td></tr>
</table>

<p>Definition at line <a class="el" href="../../df/d74/vx__khr__cnn_8h_source.html#l00161">161</a> of file <a class="el" href="../../df/d74/vx__khr__cnn_8h_source.html">vx_khr_cnn.h</a>.</p>

</div>
</div>
<a class="anchor" id="gad8a98bdea5d6620d3829b272bcccf9ab"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="../../d6/d9a/group__group__cnn.html#gad8a98bdea5d6620d3829b272bcccf9ab">vx_convolutional_network_norm_type_e</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The Convolutional Network normalization type list. </p>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><em><a class="anchor" id="ggad8a98bdea5d6620d3829b272bcccf9abaaae1a24ba2c0456e0114c8526d3e550d"></a>VX_CONVOLUTIONAL_NETWORK_NORM_SAME_MAP</em>&#160;</td><td class="fielddoc">
<p>normalization is done on same IFM </p>
</td></tr>
<tr><td class="fieldname"><em><a class="anchor" id="ggad8a98bdea5d6620d3829b272bcccf9aba224c50ee37b0e4b1a8866c06939a4d95"></a>VX_CONVOLUTIONAL_NETWORK_NORM_ACROSS_MAPS</em>&#160;</td><td class="fielddoc">
<p>Normalization is done across different IFMs. </p>
</td></tr>
</table>

<p>Definition at line <a class="el" href="../../df/d74/vx__khr__cnn_8h_source.html#l00135">135</a> of file <a class="el" href="../../df/d74/vx__khr__cnn_8h_source.html">vx_khr_cnn.h</a>.</p>

</div>
</div>
<a class="anchor" id="gab041071cfd76f089eba37379db229fb7"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="../../d6/d9a/group__group__cnn.html#gab041071cfd76f089eba37379db229fb7">vx_convolutional_network_pooling_type_e</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The Convolutional Network pooling type list. </p>
<p>kind of pooling done in pooling function </p>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><em><a class="anchor" id="ggab041071cfd76f089eba37379db229fb7a530049de61a14d590562f8840a19be2e"></a>VX_CONVOLUTIONAL_NETWORK_POOLING_MAX</em>&#160;</td><td class="fielddoc">
<p>max pooling </p>
</td></tr>
<tr><td class="fieldname"><em><a class="anchor" id="ggab041071cfd76f089eba37379db229fb7ac486a0bcf89d55a1a0bfffff548aad76"></a>VX_CONVOLUTIONAL_NETWORK_POOLING_AVG</em>&#160;</td><td class="fielddoc">
<p>average pooling </p>
</td></tr>
</table>

<p>Definition at line <a class="el" href="../../df/d74/vx__khr__cnn_8h_source.html#l00123">123</a> of file <a class="el" href="../../df/d74/vx__khr__cnn_8h_source.html">vx_khr_cnn.h</a>.</p>

</div>
</div>
<a class="anchor" id="gaea14552ad4e594583c11d8e6163557c1"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="../../d6/d9a/group__group__cnn.html#gaea14552ad4e594583c11d8e6163557c1">vx_convolutional_networks_rounding_type_e</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>The Convolutional Network down scaling size rounding type list. </p>
<p>rounding done downscaling, In convolution and pooling functions. Relevant when input size is even. </p>
<table class="fieldtable">
<tr><th colspan="2">Enumerator</th></tr><tr><td class="fieldname"><em><a class="anchor" id="ggaea14552ad4e594583c11d8e6163557c1a71594a34842bc09f8f876809d410cb52"></a>VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_FLOOR</em>&#160;</td><td class="fielddoc">
<p>floor rounding </p>
</td></tr>
<tr><td class="fieldname"><em><a class="anchor" id="ggaea14552ad4e594583c11d8e6163557c1a0548ecdc1205f9d8031fc557e41bf2f2"></a>VX_CONVOLUTIONAL_NETWORK_DS_SIZE_ROUNDING_CEILING</em>&#160;</td><td class="fielddoc">
<p>ceil rounding </p>
</td></tr>
</table>

<p>Definition at line <a class="el" href="../../df/d74/vx__khr__cnn_8h_source.html#l00110">110</a> of file <a class="el" href="../../df/d74/vx__khr__cnn_8h_source.html">vx_khr_cnn.h</a>.</p>

</div>
</div>
<h2 class="groupheader">Function Documentation</h2>
<a class="anchor" id="ga6c4f56037d6a25692ef79887fdd952af"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">vx_node vxActivationLayer </td>
          <td>(</td>
          <td class="paramtype">vx_graph&#160;</td>
          <td class="paramname"><em>graph</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_enum&#160;</td>
          <td class="paramname"><em>func</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_int32&#160;</td>
          <td class="paramname"><em>a</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_int32&#160;</td>
          <td class="paramname"><em>b</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>outputs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>[Graph] Creates a Convolutional Network Activation Layer Node. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">graph</td><td>The handle to the graph. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">inputs</td><td>The input tensor data. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">func</td><td>Non-linear function (see <code><a class="el" href="../../d6/d9a/group__group__cnn.html#ga16743849220a5ac0539ed4272b33a7dc">vx_convolutional_network_activation_func_e</a></code>). </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">a</td><td>Function parameters a. (see <code><a class="el" href="../../d6/d9a/group__group__cnn.html#ga16743849220a5ac0539ed4272b33a7dc">vx_convolutional_network_activation_func_e</a></code>). </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">b</td><td>Function parameters b. (see <code><a class="el" href="../../d6/d9a/group__group__cnn.html#ga16743849220a5ac0539ed4272b33a7dc">vx_convolutional_network_activation_func_e</a></code>). </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputs</td><td>The output tensor data. Output will have the same number of dimensions as input.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd><code> vx_node</code>. </dd></dl>
<dl class="retval"><dt>Return values</dt><dd>
  <table class="retval">
    <tr><td class="paramname">0</td><td>Node could not be created. </td></tr>
    <tr><td class="paramname">*</td><td>Node handle. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ga870c106e8ceb4c118692c6f754f75f43"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">vx_node vxConvolutionLayer </td>
          <td>(</td>
          <td class="paramtype">vx_graph&#160;</td>
          <td class="paramname"><em>graph</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>biases</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_uint32&#160;</td>
          <td class="paramname"><em>pad_x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_uint32&#160;</td>
          <td class="paramname"><em>pad_y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_uint8&#160;</td>
          <td class="paramname"><em>accumulator_bits</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_enum&#160;</td>
          <td class="paramname"><em>overflow_policy</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_enum&#160;</td>
          <td class="paramname"><em>rounding_policy</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_enum&#160;</td>
          <td class="paramname"><em>down_scale_size_rounding</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>outputs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>[Graph] Creates a Convolutional Network Convolution Layer Node. </p>
<p>This function implement Convolutional Network Convolution layer. In case the input and output <code><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a></code> are signed 16. A fixed point calculation is performed with round and saturate according to the number of accumulator bits. <br/>
round: rounding according the <code>vx_round_policy_e</code> enumeration. <br/>
saturate: A saturation according the <code>vx_convert_policy_e</code> enumeration. The saturation is done based on the accumulator_bits parameter. According the accumulator_bits, the saturation might not be performed every operation. But every a specified amount of operations, that are suspected to saturate the accumulation bits<br/>
The following equation is implemented: <br/>
 \( outputs[j,k,i] = (\sum_{l} \sum_{m,n} saturate(round(inputs[j-m,k-n,l] \times weights[m,n,l,i])))+biasses[j,k,i] \)<br/>
Where \(m,n\) are indexes on the convolution matrices. \( l\) is an index on all the convolutions per input. \( i\) is an index per output. \( j,k \) are the inputs/outputs spatial indexes. Convolution is done on the first 2 dimensions of the <code><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a></code>. Therefore, we use here the term x for the first dimension and y for the second.<br/>
before the Convolution is done, a padding of the first 2D with zeros is performed. Then down scale is done by picking the results according to a skip jump. The skip in the x and y dimension is determined by the output size dimensions. The relation between input to output is as follows: <br/>
 \( width_{output} = round(\frac{(width + 2 * pad_x - kernel_x)}{skip_x} + 1) \)<br/>
and <br/>
 \( height_{output} = round(\frac{(height + 2 * pad_y - kernel_y)}{skip_y} + 1) \)<br/>
where \(width\) is the size of the first input dimension. \(height\) is the size of the second input dimension. \(width_{output}\) is the size of the first output dimension. \(height_{output}\) is the size of the second output dimension. \(kernel_x\) and \(kernel_y\) are the convolution sizes in x and y. skip is calculated by the relation between input and output. rounding is done according to <code>vx_convolutional_network_rounding_type_e</code>. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">graph</td><td>The handle to the graph. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">inputs</td><td>The input tensor data. 3 lower dims represent a single input, and an optional 4th dimension for batch of inputs.<br/>
</td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">weights</td><td>Weights are 4d tensor with dimensions [kernel_x, kernel_y, #IFM, #OFM].<br/>
</td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">biases</td><td>The biases, which may be shared (one per ofm) or unshared (one per ofm * output location). </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">pad_x</td><td>Number of elements added at each side in the x dimension of the input. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">pad_y</td><td>Number of elements added at each side in the y dimension of the input. In fully connected layers this input is ignored. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">accumulator_bits</td><td>Is the total number of bits used during intermediate accumulation. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">overflow_policy</td><td>A <code> VX_TYPE_ENUM</code> of the <code> vx_convert_policy_e</code> enumeration. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">rounding_policy</td><td>A <code> VX_TYPE_ENUM</code> of the <code> vx_round_policy_e</code> enumeration. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">down_scale_size_rounding</td><td>Rounding method for calculating output dimensions. See <code>vx_convolutional_network_rounding_type_e</code> </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputs</td><td>The output tensor data. Output will have the same number of dimensions as input. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd><code> vx_node</code>. </dd></dl>
<dl class="retval"><dt>Return values</dt><dd>
  <table class="retval">
    <tr><td class="paramname">0</td><td>Node could not be created. </td></tr>
    <tr><td class="paramname">*</td><td>Node handle. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="gaf940a27e357e2836af6a6492148f4c93"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">vx_node vxFullyConnectedLayer </td>
          <td>(</td>
          <td class="paramtype">vx_graph&#160;</td>
          <td class="paramname"><em>graph</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>weights</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>biases</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_uint32&#160;</td>
          <td class="paramname"><em>pad</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_uint8&#160;</td>
          <td class="paramname"><em>accumulator_bits</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_enum&#160;</td>
          <td class="paramname"><em>overflow_policy</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_enum&#160;</td>
          <td class="paramname"><em>rounding_policy</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_enum&#160;</td>
          <td class="paramname"><em>down_scale_size_rounding</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>outputs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>[Graph] Creates a Fully connected Convolutional Network Layer Node. </p>
<p>This function implement Fully connected Convolutional Network layers. In case the input and output <code><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a></code> are signed 16. A fixed point calculation is performed with round and saturate according to the number of accumulator bits. <br/>
round: rounding according the <code>vx_round_policy_e</code> enumeration. <br/>
saturate: A saturation according the <code>vx_convert_policy_e</code> enumeration. The saturation is done based on the accumulator_bits parameter. According the accumulator_bits, the saturation might not be performed every operation. But every a specified amount of operations, that are suspected to saturate the accumulation bits<br/>
The equation for Fully connected layer:<br/>
 \( outputs[i] = ( \sum_{j} saturate(round(inputs[j] \times weights[j,i])))+biasses[i] \)<br/>
Where \(j\) is a index on the input feature and \(i\) is a index on the output. before the fully connected is done, a padding of the input is performed. Then down scale is done by picking the results according to a skip jump. The skip is determined by the output size dimensions. The relation between input to output is as follows: \( size_{output} = round(\frac{(size_{input} + 2 * pad)}{skip} + 1) \)<br/>
where \(size_{input}\) is the size of the input dimension. \(size_{output}\) is the size of the output dimension. skip is calculated by the relation between input and output. rounding is done according to <code>vx_convolutional_network_rounding_type_e</code>. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">graph</td><td>The handle to the graph. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">inputs</td><td>The input tensor data. 1-3 lower dims represent a single input, and all dims above dim(weights)-1 are optional for batch of inputs. Note that batch may be multidimensional. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">weights</td><td>Number of dimensions equals dim(single input)+1. Single input dims are [width, height, #IFM], with height and #IFM being optional.<br/>
</td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">biases</td><td>The biases, which may be shared (one per ofm) or unshared (one per ofm * output location). </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">pad</td><td>Number of elements added at each side in the input. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">accumulator_bits</td><td>Is the total number of bits used during intermediate accumulation. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">overflow_policy</td><td>A <code> VX_TYPE_ENUM</code> of the <code> vx_convert_policy_e</code> enumeration. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">rounding_policy</td><td>A <code> VX_TYPE_ENUM</code> of the <code> vx_round_policy_e</code> enumeration. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">down_scale_size_rounding</td><td>Rounding method for calculating output dimensions. See <code>vx_convolutional_network_rounding_type_e</code> </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputs</td><td>The output tensor data. Output will have the same number of dimensions as input. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd><code> vx_node</code>. </dd></dl>
<dl class="retval"><dt>Return values</dt><dd>
  <table class="retval">
    <tr><td class="paramname">0</td><td>Node could not be created. </td></tr>
    <tr><td class="paramname">*</td><td>Node handle. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ga2269eca24047c7b564ab7708a8420afa"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">vx_node vxNormalizationLayer </td>
          <td>(</td>
          <td class="paramtype">vx_graph&#160;</td>
          <td class="paramname"><em>graph</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_enum&#160;</td>
          <td class="paramname"><em>type</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_uint32&#160;</td>
          <td class="paramname"><em>norm_size</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_float32&#160;</td>
          <td class="paramname"><em>alpha</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_float32&#160;</td>
          <td class="paramname"><em>beta</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>outputs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>[Graph] Creates a Convolutional Network Normalization Layer Node. </p>
<p>Normalizing over local input regions. Each input value is divided by \( (1+\frac{\alpha}{n}\sum_i x^2_i)^\beta \) , where n is the number of elements to normalize across. and the sum is taken over the region centred at that value (zero padding is added where necessary). </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">graph</td><td>The handle to the graph. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">inputs</td><td>The input tensor data. 3 lower dims represent a single input with dimensions [width, height, IFM], and an optional 4th dimension for batch of inputs. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">type</td><td>Either same map or across maps (see vx_convolutional_network_norm_type_e). </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">norm_size</td><td>Number of elements to normalize across. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">alpha</td><td>Alpha parameter in the normalization equation. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">beta</td><td>Beta parameter in the normalization equation. </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputs</td><td>The output tensor data. Output will have the same number of dimensions as input.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd><code> vx_node</code>. </dd></dl>
<dl class="retval"><dt>Return values</dt><dd>
  <table class="retval">
    <tr><td class="paramname">0</td><td>Node could not be created. </td></tr>
    <tr><td class="paramname">*</td><td>Node handle. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ga0b2cc8c5e172128a3014b56a1ae7b173"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">vx_node vxPoolingLayer </td>
          <td>(</td>
          <td class="paramtype">vx_graph&#160;</td>
          <td class="paramname"><em>graph</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_enum&#160;</td>
          <td class="paramname"><em>pool_type</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_uint32&#160;</td>
          <td class="paramname"><em>pool_size_x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_uint32&#160;</td>
          <td class="paramname"><em>pool_size_y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_uint32&#160;</td>
          <td class="paramname"><em>pool_pad_x</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_uint32&#160;</td>
          <td class="paramname"><em>pool_pad_y</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">vx_enum&#160;</td>
          <td class="paramname"><em>rounding</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>outputs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>[Graph] Creates a Convolutional Network Pooling Layer Node. </p>
<p>Pooling is done on the first 2 dimensions or the <code><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a></code>. Therefore, we use here the term x for the first dimension and y for the second.<br/>
 Pooling operation is a function operation over a rectangle size and then a nearest neighbour down scale. Here we use pool_size_x and pool_size_y to specify the rectangle size on which the operation is performed. <br/>
 before the operation is done (average or maximum value). the data is padded in the first 2D with zeros. The down scale is done by picking the results according to a skip jump. The skip in the x and y dimension is determined by the output size dimensions. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">graph</td><td>The handle to the graph. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">inputs</td><td>The input tensor data. 3 lower dims represent a single input with dimensions [width, height, IFM], and an optional 4th dimension for batch of inputs. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">pool_type</td><td>Either max pooling or average pooling (see <code><a class="el" href="../../d6/d9a/group__group__cnn.html#gab041071cfd76f089eba37379db229fb7">vx_convolutional_network_pooling_type_e</a></code>). </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">pool_size_x</td><td>Size of the pooling region in the x dimension </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">pool_size_y</td><td>Size of the pooling region in the y dimension. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">pool_pad_x</td><td>Padding size in the x dimension. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">pool_pad_y</td><td>Padding size in the y dimension. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">rounding,Rounding</td><td>method for calculating output dimensions. See <code>vx_convolutional_network_rounding_type_e</code> </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputs</td><td>The output tensor data. Output will have the same number of dimensions as input. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd><code> vx_node</code>. </dd></dl>
<dl class="retval"><dt>Return values</dt><dd>
  <table class="retval">
    <tr><td class="paramname">0</td><td>Node could not be created. </td></tr>
    <tr><td class="paramname">*</td><td>Node handle. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a class="anchor" id="ga76a3a94a73992017519d7dc01e81c476"></a>
<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">vx_node vxSoftmaxLayer </td>
          <td>(</td>
          <td class="paramtype">vx_graph&#160;</td>
          <td class="paramname"><em>graph</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>inputs</em>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype"><a class="el" href="../../dd/dad/group__group__tensor.html#ga61ce36a3376598180334b1a424dcdcf9">vx_tensor</a>&#160;</td>
          <td class="paramname"><em>outputs</em>&#160;</td>
        </tr>
        <tr>
          <td></td>
          <td>)</td>
          <td></td><td></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>[Graph] Creates a Convolutional Network Softmax Layer Node. </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramdir">[in]</td><td class="paramname">graph</td><td>The handle to the graph. </td></tr>
    <tr><td class="paramdir">[in]</td><td class="paramname">inputs</td><td>The input tensor data, with number of dimensions equals dim(input batch) + 1. Softmax will be calculated per IFM. </td></tr>
    <tr><td class="paramdir">[out]</td><td class="paramname">outputs</td><td>The output tensor data. Output will have the same number of dimensions as input.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd><code> vx_node</code>. </dd></dl>
<dl class="retval"><dt>Return values</dt><dd>
  <table class="retval">
    <tr><td class="paramname">0</td><td>Node could not be created. </td></tr>
    <tr><td class="paramname">*</td><td>Node handle. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="../../doxygen.png" alt="doxygen"/></a> 1.8.6 </li>
  </ul>
</div>
</body>
</html>
